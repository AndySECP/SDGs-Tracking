{"text": "Climate change induced phenological shifts in primary pro-\nductivity result in trophic mismatches for many organisms1–4, \nwith broad implications for ecosystem structure and function. \nFor birds that have a synchronized timing of migration with \nresource availability, the likelihood that trophic mismatches \nmay generate a phenological response in migration timing \nincreases with climate change5. Despite the importance of a \nholistic understanding of such systems at large spatial and \ntemporal scales, particularly given a rapidly changing cli-\nmate, analyses are few, primarily because of limitations in the \naccess to appropriate data. Here we use 24 years of remotely \nsensed data collected by weather surveillance radar to quan-\ntify the response of a nocturnal avian migration system within \nthe contiguous United States to changes in temperature. The \naverage peak migration timing advanced in spring and autumn, \nand these changes were generally more rapid at higher lati-\ntudes. During spring and autumn, warmer seasons were pre-\ndictive of earlier peak migration dates. Decadal changes in \nsurface temperatures predicted spring changes in migratory \ntiming, with greater warming related to earlier arrivals. This \nstudy represents one of the first system-wide examinations \nduring two seasons and comprises measures from hundreds \nof species that describe migratory timing across a continent. \nOur findings provide evidence of spatially dynamic phenologi-\ncal shifts that result from climate change.", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.137725"}}
{"text": "Advances in remote-sensing technologies have enhanced our \nability to quantify phenological changes. These platforms provide \nrepeated and consistent observations over time. Most notably, \nthey have led to the development of large-scale vegetation indi-\nces20. Remote-sensing platforms for animals are rarer, but the US \nweather surveillance radar (WSR) network is emerging as a com-\nprehensive source of information about flying animals. Radars have \nrevealed numerous insights into avian migration21, but only recent \nadvances in data access and processing allow the examination of \nweather radar archives to study long-term phenological change22. \nThe use of WSRs avoids many of the sampling biases associated with \nindividual-based examinations by providing a comprehensive rep-\nresentation of the entire migration signal across the full migration  \nseason and across a considerable portion of the longitudinal breadth \nof the migration system. The methods employed in this study  \ncan be readily replicated annually, which allows for the long-term ", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.138533"}}
{"text": "monitoring of migration phenology in a consistent and rigorous \nfashion. To this end, we examined the past 24 years of radar data \ncollected over the continental United States to study the phenol-\nogy of nocturnally migrating birds. We provide the first system-\nbased indices of migration phenology and test whether migration \ntiming has shifted at these large scales. We focus on peak migra-\ntion, defined as the date by which 50% of the cumulative passage ", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.139132"}}
{"text": "studies hinted at the value of such an analysis16,26, but it was pre-\nviously impossible in the absence of a combination of advanced \nmachine learning, data accessibility and interoperability.", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.139701"}}
{"text": "The integration of additional information on species-specific \npatterns, for example, from citizen science or individual tracking, is \na priority to clarify specific mechanisms of phenology change30,31,33. \nAcquiring sufficient time-series data from these sources of informa-\ntion is challenging, but increasingly possible. Furthermore, a greater \nunderstanding of the spatial resolution of phenological change is \nimportant, as the macroclimate and microclimate may interact \nwithin regions for numerous species. Although species’ responses \nto changes in climate may vary, system-level phenology measure-\nments at large spatial and temporal scales can inform how rapidly \ndisruptions are affecting large assemblages of species. Our measures \nshow that migration systems are exhibiting widespread phenologi-\ncal changes.", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.140191"}}
{"text": "Online content\nAny methods, additional references, Nature Research reporting \nsummaries, source data, extended data, supplementary informa-\ntion, acknowledgements, peer review information; details of author \ncontributions and competing interests; and statements of data and \ncode availability are available at https://doi.org/10.1038/s41558-\n019-0648-9.", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.140617"}}
{"text": " 22. Lin, T. et al. MISTNET: Measuring historical bird migration in the US using \narchived weather radar data and convolutional neural networks. Methods \nEcol. Evol. 10, 1908–1922 (2019).", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.141072"}}
{"text": "\nLettersNaTUre ClimaTe CHaNge\nMethods\nWeather radar data acquisition. We quantified the intensity of avian migration \nto measure the phenology of migratory movements and computed the speed and \ndirection to integrate traffic flows through the night from civil dusk to civil dawn \n(the Sun 6° below the horizon). We sampled nocturnal time periods because they \ncapture the majority of migratory species that move through North America \n(~80% of migratory species)34. However, some taxonomic groups will not be \nrepresented in our analysis, including most soaring species (for example, those \nof Accipitridae, Cathartidae, Falconidae and Pelecanidae), aerial insectivores (for \nexample, those of Hirundinidae) and some diurnally migrating passerines (for \nexample, those of Fringillidae, Icteridae and Sturnidae). We used unfiltered (that is, \nlevel II) NEXRAD35 WSR data from 143 stations from spring (1 March to 15 June) \nand autumn (1 August to 15 November) that encompassed spring 1995 through \nto spring 2018. We acquired radar data through the Amazon Web Service portal \n(https://s3.amazonaws.com/noaa-nexrad-level2/index.html), extracting data every \n30 min from local sunset to sunrise. During the history of this sensor system, \nalgorithmic changes occurred, which influenced how the data are processed. \nAlthough this is not a concern for phenological analyses, because we do not make \ncomparisons of absolute magnitude across years, it is a concern when comparing \nabsolute magnitudes. For this reason, we did not include changes in abundance \nbecause we sought to analyse the entirety of the NEXRAD archive. Rosenberg \net al.36 give ten-year radar-derived comparisons of abundance.", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.141610"}}
{"text": "Clutter removal from weather radar data. Prior to constructing height profiles \nof migratory activity, we created binary masks separately for each calendar year \nand radar to remove stationary clutter (for example, buildings, wind turbines and \nterrain blockage) from the lowest elevational scan. We created masks by summing \na minimum of 100 low-elevation scans (0.5°), starting on 1 January (16:00 utc to \n18:00 utc) and continuing to 15 January. If 100 samples were unavailable by 15 \nJanuary, we expanded the window of selection until the threshold was met. We \nclassified any pixel above the 85th percentile of the summed reflectivity as clutter \nand masked it from our analyses21.", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.142489"}}
{"text": "Precipitation removal from WSR. To remove weather contamination, we trained \na deep convolutional neural network (CNN) to segment regions of precipitation \nfrom the biology in WSR volume scans and set the reflectivity of any pixel to zero \nif it was classified as precipitation22. Precipitation and migratory movements tend \nto be mutually exclusive, with precipitation, especially heavy precipitation, halting \nthe movement of migrants37,38. We trained the CNN using scans sampled at 30 min \nintervals for the first 3 h after the local sunset for all WSR stations in April, May, \nSeptember and October 2014–2016 (239,128 scans in total). We assigned per-pixel \ntraining labels using polarimetric variables: if the correlation coefficient exceeded \n0.95, reflectivity was classified as rain, otherwise it was classified as biological. The \nCNN used an FCN8 architecture39 with a VGG-16 backbone40 modified to the \ndimensions of the radar data, and was trained by back-propagation41 and stochastic \ngradient descent42. The trained CNN classifies pixels using only legacy radar \nvariables (for example, reflectivity, radial velocity and spectrum width), and may \nbe run on any historical radar scan. We evaluated the performance on manually \nsegmented scans that were both historically and geographically representative; the \nCNN retained 95.9% of all the biomass (summed reflectivity of the pixels classified \nas biology) with a false-positive rate of 1.3%.", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.143284"}}{"text": "For generations, climate scientists have educated the public \nthat ‘weather is not climate’, and climate change has been \nframed as the change in the distribution of weather that slowly \nemerges from large variability over decades1–7. However, \nweather when considered globally is now in uncharted terri-\ntory. Here we show that on the basis of a single day of globally \nobserved temperature and moisture, we detect the fingerprint \nof externally driven climate change, and conclude that Earth as \na whole is warming. Our detection approach invokes statisti-\ncal learning and climate model simulations to encapsulate the \nrelationship between spatial patterns of daily temperature and \nhumidity, and key climate change metrics such as annual global \nmean temperature or Earth’s energy imbalance. Observations \nare projected onto this relationship to detect climate change. \nThe fingerprint of climate change is detected from any single \nday in the observed global record since early 2012, and since \n1999 on the basis of a year of data. Detection is robust even \nwhen ignoring the long-term global warming trend. This \ncomplements traditional climate change detection, but also \nopens broader perspectives for the communication of regional \nweather events, modifying the climate change narrative: while \nchanges in weather locally are emerging over decades, global \nclimate change is now detected instantaneously.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.137929"}}
{"text": "Formal D&A of externally forced signals in Earth’s observed \nclimate relies on spatial patterns, so-called ‘fingerprints’, that \nencapsulate expected physical climate responses to external forc-\ning7. We extend an established fingerprint detection method11,13,17 \nby incorporating a regression method that improves the separa-\ntion of signal and noise based on daily data (see Methods and \nExtended Data Fig. 1). First, we define two key climate change \nmetrics, annual global mean temperature (AGMT) and a decadal \naverage of Earth’s energy imbalance (EEI), that serve as target \nvariables for detection. AGMT characterizes climate warming \nfrom a surface perspective, used in policy assessments18,19, while \nEEI characterizes the state of climate change from a more physi-\ncally motivated energy balance perspective20,21. Second, we train \nregularized linear regression models, using ridge regression, \nthat predict each of the two targets from daily spatial patterns of \nsurface temperature and/or humidity for each month and based  \non model simulations in the Coupled Model Intercomparison \nProject Phase 5 (CMIP5) multi-model archive driven by external \nforcing. This step yields fingerprints as maps of regression coef-\nficients that encapsulate the relationship between global patterns \nof daily weather and each target metric while reducing ‘noise’ in \nregions with large internal variability or model disagreement. \nFinally, and similarly to established detection methods11,13,17, \nwe project observations onto the model-derived fingerprints to \nobtain a prediction of each target metric. We then assess whether \nexternally forced climate change is detected by testing against the \nnull hypothesis that the predicted test statistic is indistinguishable \nfrom natural variability.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.138798"}}
{"text": "To understand the spatial signature of externally forced climate \nchange in daily data, we discuss AGMT fingerprints and their sea-\nsonal cycle, which reveals key climatological features (Fig. 2a,b). \nFirst, regression coefficients are positive throughout the globe, indi-\ncating that the fingerprints pick up a global warming signal. Second, \ncoefficients are larger over the oceans than over land, and larger over \nthe tropics than over mid- or high latitudes. These features illustrate \nthat signal-to-noise ratios of forced climate change in daily data \nare higher over the oceans than over land, and higher in the trop-\nics than over mid- or high-latitudes, consistent with our physical  \nunderstanding of forced changes on longer timescales23,24. Polar \nregions do not receive much weight in the fingerprints despite very ", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.139358"}}
{"text": "Fig. 1 | Warming of daily temperatures experienced at the local and global scale. a–d, The distribution of local (a,c) and global (b,d) daily temperatures in \nthe National Centers for Environmental Prediction (NCEP) 1 reanalysis dataset (a,b) and in the CMIP5 multi-model archive (c,d). The local histograms are \narea-weighted on the basis of temperature anomalies from all grid cells and all seasons relative to their respective 1979–2005 mean seasonal cycle.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.139941"}}
{"text": "Fig. 3 | agMt estimates from models, reanalyses and observations. a–f, Time series of AGMT (bold lines) for the CMIP5 multi-model mean (a,b), \nreanalysis datasets (c,d) and observations (e,f). Estimates of AGMT based on each single day (a–e, in e for daily observations that are experimental and \nshown for illustration alone, see Methods) and single months (e,f) are shown. All estimates are derived on the basis of the spatial pattern of daily/monthly \ntemperatures with the mean signal included (a,c,e); and based on the spatial pattern of daily/monthly temperature and humidity with the mean signal \nremoved (b,d,f). The boxplots show the 2.5th to 97.5th percentile of the distribution of the AGMT test statistic for the 1870–1950 historical period in \nCMIP5 with each model’s contribution weighted equally. avg., average; obs., observed; pred., predicted.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.141349"}}
{"text": "CMIP5 models predict that forced climate change can be \ndetected at a daily basis from the early 2000s onwards, where the \nrange of daily AGMT estimates emerges from natural variability \n(Fig. 3a,b). The reanalyses average indicates that since the end \nof 2001 the majority of days in any given year lies outside natu-\nral variability irrespective of whether the global mean trend sig-\nnal has been removed or not (Fig. 3; 97.7% (‘mean included’) and \n83.6% (‘mean removed’) of days detected individually from 2001 \nup to August 2019). Since late March 2012, every single day is \ndetected individually (Fig. 4a, ‘mean included’), and all but 3.5% \nof days in the ‘mean removed’ case (Fig. 4b). In observations, every \nmonth individually is outside natural variability since 2001 with  \ndata available up to December 2018 (‘mean included’). In an  \nexperimental daily observational dataset, every day would be  ", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.142124"}}
{"text": "Fig. 4 | Emergence of externally forced climate change in ‘global weather’ from the noise of natural variability. a–d, The fraction of days in reanalysis \ndatasets (a,b), daily observations (a, experimental and illustrative alone) and months in observations (c,d) that have emerged from daily natural variability \n(that is, exceeding the 97.5th percentile in the CMIP5 1870–1950 distribution). The estimates are based on the spatial pattern of temperature with the \nmean signal included (a,c,e) and the joint spatial pattern of temperature and land humidity with the mean signal removed (b,d,f). e,f, The emergence of \nobservations based on individual years.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.142788"}}
{"text": "\nLetters NaTurE ClimaTE CHaNgE\nindividually detected since about 2008 (Fig. 3e, ‘mean included’). \nIf the mean is removed, only six months (7.3% with humidity data \navailable up to December 2017) fall within natural variability since \nthe end of 2010 (Fig. 3f). As the test statistic varies characteristi-\ncally across timescales, global climate change is probably detect-\nable also at shorter subdaily or even instantaneous timescales \n(Extended Data Fig. 3). Overall, the conclusion that a forced cli-\nmate change is detected with >97.5% confidence in any individ-\nual day or month since early 2012 is robust across any of the five \nreanalysis and observational datasets.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.143382"}}{"text": "Healthcare applications supported by the Internet of Things enable personalized monitoring of a patient in everyday settings.\nSuch applications often consist of battery-powered sensors coupled to smart gateways at the edge layer. Smart gateways offer\nseveral local computing and storage services (e.g., data aggregation, compression, local decision making), and also provide\nan opportunity for implementing local closed-loop optimization of different parameters of the sensor layer, particularly\nenergy consumption. To implement efficient optimization methods, information regarding the context and state of patients\nneed to be considered to find opportunities to adjust energy to demanded accuracy. Edge-assisted optimization can manage\nenergy consumption of the sensor layer but may also adversely affect the quality of sensed data, which could compromise the\nreliable detection of health deterioration risk factors. In this article, we propose two approaches: myopic and Markov decision\nprocesses (MDPs)—to consider both energy constraints and risk factor requirements for achieving a twofold goal: energy\nsavings while satisfying accuracy requirements of abnormality detection in a patient’s vital signs. Vital signs, including heart\nrate, respiration rate, and oxygen saturation, are extracted from a photoplethysmogram signal and errors of extracted features\nare compared to a ground truth that is modeled as a Gaussian distribution. We control the sensor’s sensing energy to minimize\nthe power consumption while meeting a desired level of satisfactory detection performance. We present experimental results\non realistic case studies using a reconfigurable photoplethysmogram sensor in an IoT system, and show that compared to\nnonadaptive methods, myopic reduces an average of 16.9% in sensing energy consumption with the maximum probability of\nabnormality misdetection on the order of 0.17 in a 24-hour health monitoring system. In addition, over 4 weeks of monitoring,\nwe demonstrate that our MDP policy can extend the battery life on average of more than 2x while fulfilling the same average\nprobability of misdetection compared to the myopic method. We illustrate results comparing myopic, MDP, and nonadaptive\nmethods to monitor 14 subjects over 1 month.", "meta": {"corpora_id": "acm4.2020-05-22 15:28:34.137970"}}
{"text": "Interconnected low-cost and miniaturized wearable sensors are increasingly being proposed as an integral part\nof next-generation health care systems [1–3]. In a recent trend in this class of technological problems, these\nsensors are integrated within the Internet of Things (IoT) infrastructure to build distributed systems capable of\nperforming highly complex processing of the acquired signals. Of particular interest is the recently proposed\nedge computing paradigm [4, 5], where compute-capable devices—the edge servers—placed at the edge of the\nnetwork take over data processing tasks generated by interconnected devices. The low latency of the wireless\nlinks connecting the devices to the edge server makes these architectures capable of supporting time-sensitive\napplications [6, 7].", "meta": {"corpora_id": "acm4.2020-05-22 15:28:34.138873"}}
{"text": "Based on this general concept and architecture, we fully develop a specific application whose objective is\nto detect abnormalities in vital signs extracted from PPG sensors [8, 9]. PPG is a low-cost and miniaturized\noptical sensor widely used in medical and wearable sensors (e.g., fitness trackers, smart rings, smart earrings),\nwhich can continuously capture several vital signs such as heart rate, heart rate variability, respiration rate, and\nblood oxygenation [10]. Based on real-world data, we build a model for normal and abnormal signals, and define\ncorresponding regions in a feature space. In this context, the edge server will detect the current activity of the\nmonitor person and adjust the power used by the PPG sensor to ensure that the misdetection probability is below\na predetermined threshold while maximizing the lifetime of the sensor.", "meta": {"corpora_id": "acm4.2020-05-22 15:28:34.139675"}}
{"text": "The rest of the article is organized as follows. In Section 2, we provide background on the addressed problem\nand discuss related work. Section 3 describes the layered architecture of the system. The monitoring and detection\nframeworks are presented in Section 4. Section 5 discusses the process of extracting vital signs from a PPG signal\nand setup for collecting activity related data from various subjects. Section 6 presents and discusses numerical\nresults, and Section 7 concludes the article.", "meta": {"corpora_id": "acm4.2020-05-22 15:28:34.140307"}}
{"text": "the augmented (smart) version of these gateways [6] and their processing power to implement the core part of\nthe proposed context-aware control: to analyze the incoming data, plan for future configurations of the sensing\ndevice, and send the configuration data to the sensor layer. We remark that the three-tier architecture introduces\nchallenges. First, a coherent management of communications and task allocation among the layers necessitates\ncareful design. Then, the distribution of tasks inevitably introduces a delay in the propagation of information\nthroughout the system. Finally, a three-layer design introduces security and privacy concerns. Herein, we estab-\nlish a fast control loop using the edge server, which is connected to the sensor through a one-hop wireless link.\nWe remark that this strategy grants significant performance gain to the sensor, which has limited computation\nand energy resources.", "meta": {"corpora_id": "acm4.2020-05-22 15:28:34.140849"}}
{"text": "As shown in Figure 1, the edge server (e.g., a smart gateway) receives the sensing device information together\nwith the PPG and body acceleration signals and locates the subject’s health status, activity level, and state of the\nsensing device. It then determines the sensing parameters and sends the new configuration to the sensor node.\nNote that the overall processing involves some additional steps, such as filtering, encryption, and compression,\nand sends the preprocessed data to the cloud server, which builds a statistical model of the person’s temporal\nsequence of activities. The sensing and edge layer are connected through a wireless link, which supports data\ntransfer upstream from the sensors to the edge server and, in the proposed framework, control downstream from\nthe edge server to the sensors.", "meta": {"corpora_id": "acm4.2020-05-22 15:28:34.141581"}}
{"text": "We use a virtual private server as a cloud server. Similar to an edge device, the cloud server runs an Apache\nweb server on an Ubuntu Linux operating system (OS). A service on the Apache web server is responsible for\nreceiving data from the edge devices and storing them in the OS file system. A MySQL database server stores\nuser information and the index of their related files. These services gradually create a behavior and medical\nhistory of the user. We process the stored data through another service to create the weekly Markov model for\neach user. The server updates the edge device periodically with the latest users’ Markov models.", "meta": {"corpora_id": "acm4.2020-05-22 15:28:34.142773"}}
{"text": "In this section, we first describe in detail the system setup in Section 5.1, and in Section 5.2, we describe the\nclinical trial at the base of this study and its output data.", "meta": {"corpora_id": "acm4.2020-05-22 15:28:34.143626"}}
{"text": "We now provide some specifics of the IoT system we developed. The sensing device, besides the actual PPG\nsensor, is equipped with a microcontroller to read data from the sensor, a Flash memory to store data temporar-\nily, and a wireless data transmission module to send the recorded data. We use the ESP8266-12E board, which\nintegrates all mentioned components in a single board. In the specific board we used, a full TCP/IP stack is in-\ntegrated with an L106 32-bit RISC microprocessor core running at 80 MHz, with 96 KB of on-chip SRAM and\n4 MB of external Flash memory. For the sensor, we use a MAX-REFDES117 PPG sensor board, which is highly\nconfigurable and provides a digitized signal through I2C communication. We program the microprocessor using\nthe C programming language to configure the sensor board internal registers. The sensing device connects to the\nedge device via a WiFi connection. It has a permanent configuration memory that records the user information\nand credentials of three different WiFi networks. At the beginning of the operation, the sensor node connects", "meta": {"corpora_id": "acm4.2020-05-22 15:28:34.144308"}}{"text": "Online content\nAny methods, additional references, Nature Research reporting \nsummaries, source data, extended data, supplementary informa-\ntion, acknowledgements, peer review information; details of author \ncontributions and competing interests; and statements of data and \ncode availability are available at https://doi.org/10.1038/s41558-\n019-0635-1.", "meta": {"corpora_id": "nature5.2020-05-22 15:28:34.138670"}}
{"text": "Acknowledgements\nThe authors thank M. Mori for providing data from the MIROC simulations and \nfor useful discussions. We acknowledge the individuals and modelling groups that \ncontributed to the Facility for Climate Assessments (FACTS) multimodel dataset and the \nCESM Large Ensemble Project.", "meta": {"corpora_id": "nature5.2020-05-22 15:28:34.139529"}}
{"text": "\nmatters arisingNature Climate ChaNge\nmethods\nIn Fig. 1a,c,e, we use the exact same data as Mori et al.1. Briefly, observational \nresults come from the ERA-Interim reanalysis for the period 1979–2014 and \nmodelled results come from seven AGCM simulations in which observed sea \nsurface temperatures and sea ice have been specified. Further details on these \nmodel simulations can be found in Mori et al.1. In Fig. 1b,d,f, we also analyse two \nexperiments performed as part of the Community Earth System Model (CESM) \nLarge Ensemble project18. The first is a 200-year section (years 401–600) of a pre-\nindustrial control run of the CESM configured with the Community Atmosphere \nModel version 5 (CAM5). CESM–CAM5 is a global coupled climate model with \na horizontal resolution of approximately 1° in all model components. The second \nadditional experiment is a 200-year simulation with CAM5 in which sea surface \ntemperatures and sea ice were specified from years 401–600 of the parent  \nCESM–CAM5 simulation. External forcing is the same in both simulations. \nInitially in Fig. 1a, we employed the same methodologies as Mori et al.1 to  \ncalculate the WACE pattern, the WACE time series and the Barents–Kara sea-ice \nindex (see Mori et al.1 for details), with one exception. We computed the WACE \nand Barents–Kara sea-ice time series from monthly averages, whereas Mori et al.1 \nused winter averages. The purpose of this modification is to facilitate subseasonal \nlead–lag correlations. Projecting near-surface temperatures for December,  \nJanuary and February separately onto the winter mean WACE pattern produced \nmonthly WACE time series. Figure 1a shows the total and sea-ice-driven variance \nfor the three winter months combined. To construct Fig. 1b, we performed an \nanalogous analysis but substituted data from ERA-Interim with that from  \nCESM–CAM5. More specifically, we derived the WACE pattern as the leading \nmode of covariability from singular value decomposition applied to the  \nCESM–CAM5 and CAM5 simulations. The r2 value was then calculated by \ncorrelating the corresponding WACE time series with the Barents–Kara sea-ice \nindex from CESM–CAM5. In this so-called perfect model comparison19,20,  ", "meta": {"corpora_id": "nature5.2020-05-22 15:28:34.140206"}}
{"text": "Healthcare applications supported by the Internet of Things enable personalized monitoring of a patient in everyday settings.\nSuch applications often consist of battery-powered sensors coupled to smart gateways at the edge layer. Smart gateways offer\nseveral local computing and storage services (e.g., data aggregation, compression, local decision making), and also provide\nan opportunity for implementing local closed-loop optimization of different parameters of the sensor layer, particularly\nenergy consumption. To implement efficient optimization methods, information regarding the context and state of patients\nneed to be considered to find opportunities to adjust energy to demanded accuracy. Edge-assisted optimization can manage\nenergy consumption of the sensor layer but may also adversely affect the quality of sensed data, which could compromise the\nreliable detection of health deterioration risk factors. In this article, we propose two approaches: myopic and Markov decision\nprocesses (MDPs)—to consider both energy constraints and risk factor requirements for achieving a twofold goal: energy\nsavings while satisfying accuracy requirements of abnormality detection in a patient’s vital signs. Vital signs, including heart\nrate, respiration rate, and oxygen saturation, are extracted from a photoplethysmogram signal and errors of extracted features\nare compared to a ground truth that is modeled as a Gaussian distribution. We control the sensor’s sensing energy to minimize\nthe power consumption while meeting a desired level of satisfactory detection performance. We present experimental results\non realistic case studies using a reconfigurable photoplethysmogram sensor in an IoT system, and show that compared to\nnonadaptive methods, myopic reduces an average of 16.9% in sensing energy consumption with the maximum probability of\nabnormality misdetection on the order of 0.17 in a 24-hour health monitoring system. In addition, over 4 weeks of monitoring,\nwe demonstrate that our MDP policy can extend the battery life on average of more than 2x while fulfilling the same average\nprobability of misdetection compared to the myopic method. We illustrate results comparing myopic, MDP, and nonadaptive\nmethods to monitor 14 subjects over 1 month.", "meta": {"corpora_id": "acm3.2020-05-22 15:28:34.141605"}}
{"text": "Interconnected low-cost and miniaturized wearable sensors are increasingly being proposed as an integral part\nof next-generation health care systems [1–3]. In a recent trend in this class of technological problems, these\nsensors are integrated within the Internet of Things (IoT) infrastructure to build distributed systems capable of\nperforming highly complex processing of the acquired signals. Of particular interest is the recently proposed\nedge computing paradigm [4, 5], where compute-capable devices—the edge servers—placed at the edge of the\nnetwork take over data processing tasks generated by interconnected devices. The low latency of the wireless\nlinks connecting the devices to the edge server makes these architectures capable of supporting time-sensitive\napplications [6, 7].", "meta": {"corpora_id": "acm3.2020-05-22 15:28:34.142422"}}
{"text": "Based on this general concept and architecture, we fully develop a specific application whose objective is\nto detect abnormalities in vital signs extracted from PPG sensors [8, 9]. PPG is a low-cost and miniaturized\noptical sensor widely used in medical and wearable sensors (e.g., fitness trackers, smart rings, smart earrings),\nwhich can continuously capture several vital signs such as heart rate, heart rate variability, respiration rate, and\nblood oxygenation [10]. Based on real-world data, we build a model for normal and abnormal signals, and define\ncorresponding regions in a feature space. In this context, the edge server will detect the current activity of the\nmonitor person and adjust the power used by the PPG sensor to ensure that the misdetection probability is below\na predetermined threshold while maximizing the lifetime of the sensor.", "meta": {"corpora_id": "acm3.2020-05-22 15:28:34.143321"}}
{"text": "The rest of the article is organized as follows. In Section 2, we provide background on the addressed problem\nand discuss related work. Section 3 describes the layered architecture of the system. The monitoring and detection\nframeworks are presented in Section 4. Section 5 discusses the process of extracting vital signs from a PPG signal\nand setup for collecting activity related data from various subjects. Section 6 presents and discusses numerical\nresults, and Section 7 concludes the article.", "meta": {"corpora_id": "acm3.2020-05-22 15:28:34.144051"}}
{"text": "the augmented (smart) version of these gateways [6] and their processing power to implement the core part of\nthe proposed context-aware control: to analyze the incoming data, plan for future configurations of the sensing\ndevice, and send the configuration data to the sensor layer. We remark that the three-tier architecture introduces\nchallenges. First, a coherent management of communications and task allocation among the layers necessitates\ncareful design. Then, the distribution of tasks inevitably introduces a delay in the propagation of information\nthroughout the system. Finally, a three-layer design introduces security and privacy concerns. Herein, we estab-\nlish a fast control loop using the edge server, which is connected to the sensor through a one-hop wireless link.\nWe remark that this strategy grants significant performance gain to the sensor, which has limited computation\nand energy resources.", "meta": {"corpora_id": "acm3.2020-05-22 15:28:34.144775"}}{"text": "Emissions of nitrous oxide (N2O) from the world’s river net-\nworks constitute a poorly constrained term in the global N2O \nbudget1,2. This N2O component was previously estimated as \nindirect emissions from agricultural soils3 with large uncer-\ntainties4–10. Here, we present an improved model representa-\ntion of nitrogen and N2O processes of the land–ocean aquatic \ncontinuum11 constrained with an ensemble of 11 data prod-\nucts. The model–data framework provides a quantification \nfor how changes in nitrogen inputs (fertilizer, deposition and \nmanure), climate and atmospheric CO2 concentration, and \nterrestrial processes have affected the N2O emissions from \nthe world’s streams and rivers during 1900–2016. The results \nshow a fourfold increase of global riverine N2O emissions \nfrom 70.4 ± 15.4 Gg N2O-N yr−1 in 1900 to 291.3 ± 58.6 Gg \nN2O-N yr−1 in 2016, although the N2O emissions started to \ndecline after the early 2000s. The small rivers in head water \nzones (lower than fourth-order streams) contributed up to \n85% of global riverine N2O emissions. Nitrogen loads on \nheadwater streams and groundwater from human activities, \nprimarily agricultural nitrogen applications, play an important \nrole in the increase of global riverine N2O emissions.", "meta": {"corpora_id": "nature2.2020-05-22 15:28:34.138857"}}
{"text": "the subgrid routine processes without conducting model simulation \nat fine resolution (Supplementary Fig. 1b). To quantify the influ-\nences of natural and human activities on riverine N2O emissions, \nthe model was driven by many factors including climate (shortwave \nradiation, precipitation, air temperature, maximum temperature \nand minimum temperature), land use and land cover, and nitro-\ngen inputs (fertilizer, deposition, manure and sewage) from 1900 \nto 2016 (Supplementary Fig. 2). The simulated river discharges and \nnitrate (NO3−), ammonium (NH4+) and dissolved organic nitrogen \nconcentrations were calibrated using observations from 50 large \nriver basins across the globe (Supplementary Fig. 3). The simulated \ngroundwater-dissolved N2O concentration and riverine-dissolved \nN2O concentration agreed well with observations both spatially \nand temporally (Supplementary Table 1 and Supplementary  \nFigs. 4–6). To assess the uncertainty of riverine N2O emissions,  \nsix datasets of N inputs and five estimates of river surface area \n(Methods and Supplementary Fig. 7) were used to drive the model. \nThe average of the ensemble N2O outputs was taken as the best \nestimation. Moreover, factorial experiments (Fig. 1b) were con-\nducted to attribute the contribution of each factor (climate, CO2, \nfertilizer, manure and N deposition) to riverine N2O emissions \n(Supplementary Table 2).", "meta": {"corpora_id": "nature2.2020-05-22 15:28:34.139795"}}
{"text": "biogeochemical processes; measurements and driving data also \nneed to improve. In particular, model parameters were the largest \nsource of uncertainty, followed by river surface area and nitrogen \ninputs (Supplementary Fig. 7). A rainfall event can increase the sur-\nface area of the first-order streams greatly but the high flow veloci-\nties make surface area prediction difficult22. Gas exchange rates also \nshow large variations by streams which requires further investiga-\ntion30. We simulated the N2O production from nitrification and \ndenitrification using a Q10-based empirical method, in which water \ntemperature is the only determinant (the first-order mechanism). \nAlthough some deficits exist in this method to explicitly account \nfor other critical factors, such as carbon availability, microbe activ-\nity and the level of dissolved oxygen (Supplementary Fig. 7), the \nparameterization of nitrification and denitrification rates at the \nreference temperature does implicitly consider impacts of other \nfactors. Moreover, the method is further validated by this study \n(Supplementary Table 1)25. Currently, the process-based subsurface \nhydrodynamic model requires variables such as thickness or extent \nof the hyporheic zone, hyporheic denitrification rate21. However, \nthese variables remain highly uncertain due to the lack of field mea-\nsurements globally. Therefore, the rigorous interaction between \nprocess-based modelling and field experimentation will be essential \nto reduce estimate uncertainty in the lateral N2O emission for clos-\ning the global N2O budget.", "meta": {"corpora_id": "nature2.2020-05-22 15:28:34.140510"}}
{"text": "Online content\nAny methods, additional references, Nature Research reporting \nsummaries, source data, extended data, supplementary informa-\ntion, acknowledgements, peer review information; details of author \ncontributions and competing interests; and statements of data and \ncode availability are available at https://doi.org/10.1038/s41558-\n019-0665-8.", "meta": {"corpora_id": "nature2.2020-05-22 15:28:34.141312"}}
{"text": "\nLetters Nature Climate ChaNge\nmethods\nWe collected site-level observations of dissolved N2O concentration, riverine \nN2O flux and groundwater N2O concentration from literature, to calibrate and \nvalidate our riverine N2O model within the DLEM framework (Supplementary \nInformation). Meanwhile, six collected datasets of nitrogen input and an \nestimate of river water surface area were used to evaluate the input data-induced \nuncertainties in riverine N2O emissions. The detailed information on the model \nand input data are given as follows.", "meta": {"corpora_id": "nature2.2020-05-22 15:28:34.142131"}}
{"text": "The annual land use/land cover change data were derived from a potential \nnatural vegetation map (synergetic land cover product31) and a prescribed \ncropland area dataset from the history database of the global environment v.3.2 \n(HYDE 3.2, ftp://ftp.pbl.nl/hyde/). The daily climate variables (precipitation, \nmean temperature, maximum temperature, minimum temperature and shortwave \nradiation) were obtained from the CRU–NCEP dataset (https://vesg.ipsl.upmc.fr)  \nfor 1900–2016. Annual atmospheric CO2 concentration from 1900 to 2015 was \nobtained from the NOAA GLOBALVIEW-CO2 dataset (https://www.esrl.noaa.\ngov). Long-term atmospheric N2O concentration was obtained from the AGAGE \ndataset (https://agage.mit.edu/data/agage-data).", "meta": {"corpora_id": "nature2.2020-05-22 15:28:34.142963"}}
{"text": "Model simulations were driven by multiple data sources of N deposition and \nN fertilizer use, including two datasets of N deposition at a resolution of 0.5° × 0.5° \nfrom the atmospheric chemistry and climate model intercomparison project \n(ACCMIP)32 and chemistry–climate model initiative (CCMI) models33. Three \ndatasets of agricultural N fertilizer use were obtained from refs. 34–36.A spatially \nexplicit dataset of manure N application on global croplands developed by Zhang \net al.37 was also used to drive DLEM. Additional detailed information about \nnitrogen inputs can be found in Supplementary information and other published \ndocuments32–36.", "meta": {"corpora_id": "nature2.2020-05-22 15:28:34.143650"}}
{"text": "The ACCMIP deposition data and the fertilizer data34 were selected as the \nnitrogen inputs for our attribution analysis experiment (Fig. 1b and Supplementary \nTable 2) and model calibration (Supplementary Figs. 3–6). The historical global \ngross domestic product and population data obtained from the intersectoral \nimpact model intercomparison project (ISIMIP, https://esg.pik-potsdam.de/search/\nisimip/) are used for estimating sewage N exports, using the method proposed by \nVan Drecht et al.38.", "meta": {"corpora_id": "nature2.2020-05-22 15:28:34.144390"}}
{"text": "Uncertainty analysis. We have evaluated three main sources of uncertainty in \nestimating riverine N2O emissions: (1) N input data-induced uncertainty, (2) \nriver surface area-induced uncertainty and (3) parameter-induced uncertainty. To \nevaluate uncertainty of riverine N2O emissions induced by nitrogen input data, \nwe carried out four simulations by using different nitrogen input datasets. We first \nchose ACCMIP deposition32 data and fertilizer data34 as a benchmark and then \nsubstituted N fertilizer data with datasets35,36 for two other separate experiments. \nWe also replaced ACCMIP N deposition data with CCMI N deposition data33 for \nrunning another simulation.", "meta": {"corpora_id": "nature2.2020-05-22 15:28:34.145239"}}{"text": "Timely detection of an individual’s stress level has the potential to improve stress management, thereby reducing the risk\nof adverse health consequences that may arise due to mismanagement of stress. Recent advances in wearable sensing have\nresulted in multiple approaches to detect and monitor stress with varying levels of accuracy. The most accurate methods,\nhowever, rely on clinical-grade sensors to measure physiological signals; they are often bulky, custom made, and expensive,\nhence limiting their adoption by researchers and the general public. In this article, we explore the viability of commercially\navailable off-the-shelf sensors for stress monitoring. The idea is to be able to use cheap, nonclinical sensors to capture phys-\niological signals and make inferences about the wearer’s stress level based on that data. We describe a system involving a\npopular off-the-shelf heart rate monitor, the Polar H7; we evaluated our system with 26 participants in both a controlled lab\nsetting with three well-validated stress-inducing stimuli and in free-living field conditions. Our analysis shows that using\nthe off-the-shelf sensor alone, we were able to detect stressful events with an F1-score of up to 0.87 in the lab and 0.66 in the\nfield, on par with clinical-grade sensors.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.138265"}}
{"text": "Existing methods commonly used by behavioral psychologists to quantify and monitor stress levels, such as\nthe Perceived Stress Scale (PSS) [16], have two limitations: (1) they rely on self-report data, and (2) they are\nwindows into moments in time rather than continuous monitors. Moreover, these methods require respondents\nto stop their ongoing activity to fill in the questionnaire. These limitations, although acceptable for retrospective\nstudies of stress, make prospective studies and real-time interventions impossible. For real-time interventions,\nwe need to be able to continuously measure and monitor an individual’s stress level. One approach to enable\nthis sort of real-time measurement and feedback is through the use of wearable sensors.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.139093"}}
{"text": "With recent advancements in sensor and wearable technologies, it is now possible to continuously collect and\nstream physiological signals for near-real-time analysis. Indeed, researchers are beginning to make progress on\ncontinuous and passive measurement of stress, both in the laboratory and in free-living settings [25, 28, 29, 32,\n33, 38, 42, 48]. Although this prior work introduces and studies a variety of wearable devices and sensors to\ncapture physiological data with a focus on detecting or predicting stress (or stressful events), it relies on custom-\nmade or clinical-grade sensors, which are often bulky, uncomfortable, inaccessible, and/or expensive, making\nthem unappealing or out of reach for many. These limitations prevent large-scale adoption of such sensors by\n(1) researchers who want to observe participant stress in real or near-real time; (2) researchers who want to\nstudy interventions and their effect on other behaviors such as anxiety, smoking cessation or drug abuse; and\n(3) consumers who want to monitor their stress level beyond the clinical setting, in free-living conditions. Toward\nmaking accessible and affordable wearable sensors for stress monitoring possible, in this work we aim to answer\nthe following question: Can we use a commodity device to accurately detect stress?", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.139615"}}
{"text": "• We compare a variety of data processing methods and their effect on the accuracy of stress inference\nusing a commodity sensor. We demonstrate that some of the typical preprocessing steps used in prior\nwork do not perform equally well for commodity devices.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.140245"}}
{"text": "• We make recommendations about the data processing pipeline for the task of stress detection. Although\nour aim was to test applicability for commodity sensors, we show our pipeline also applies to custom-built\nsensors (a galvanic skin response (GSR) sensor) as well. We believe that the recommendations made can", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.140768"}}
{"text": "• We propose a novel two-layer method for detecting stress, which can account for a participant’s previous\nstress level while determining the current stress level. We show that using the two-layer approach leads to\na notable improvement in stress detection performance. Using only the data from Polar H7 (heart rate and\nR-R interval), we saw an F1-score of 0.88 and 0.66 for detecting stress in the lab and free-living conditions,\nrespectively.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.141560"}}
{"text": "Improvements in sensors and sensing capabilities over the years have led to a spectrum of prior work in stress\ndetection and assessment. There are multiple methods that have been used for “contactless” stress measurement,\nsuch as using the user’s voice [35], or using accelerometer-based contextual modeling [24], or phone usage\ndata like Bluetooth and Call/SMS logs [10]; however, we focus on related works using wearable devices for\nphysiology-based stress measurement. Although contactless approaches have some advantages, they also have\nseveral limitations, such as lack of continuous assessment, dependency on personalized models, or the need for\nextensive training across various situations.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.142361"}}
{"text": "Muaremi et al. [40] used a combination of the Zephyr BioHarness 3.0 [57] and an Empatica E3 [23] for moni-\ntoring stress while sleeping. Gjoreski et al. [26] used both the Empatica E3 and E4 [19] to detect stress in a lab and\nan unconstrained field (free-living) setting. Sano and Picard used the Affectiva Q Sensor along with smartphone\nusage data to predict the PSS scores at the end of the experiment [47]. In all of these works, the sensors they\nused are marketed as “highquality” or “clinicalquality” physiological sensors and hence are too expensive2 to be", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.143222"}}
{"text": "Although the Polar H7 has also been used by Egilmez et al. [18] in UStress, they used it to just get the heart\nrate values (beats per minute) to act as a supplement to their custom GSR sensor for stress prediction in the lab\nsetting. The authors then compared the differences in prediction results by using heart rate information obtained\nfrom a chest-strap sensor (Polar H7) and a smartwatch (from LG). Our work, however, gives insights into the\nfeasibility of using the heart rate and R-R interval data from just a commodity sensor (Polar H7) for being able\nto detect and predict stress both in a controlled lab environment and an unconstrained field scenario.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.144175"}}
{"text": "We summarize all of the previous work mentioned in this section in Table 1. We report the type of envi-\nronment/situation(s) where the study was conducted, the type of data collected in those studies, the types of\nsensors/devices used, the number of participants, and the results obtained by the authors.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.144922"}}
{"text": "In what follows, we describe the devices used, the lab and field procedure, and the data collected.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.145685"}}
{"text": "In addition to the heart rate monitor, for the field study, the participants wore the Amulet wrist device [31]\nto collect activity data and trigger ecological momentary assessment (EMA) prompts. The Amulet also served\nas the data hub to collect the data from the heart rate monitor using Bluetooth Low Energy (BLE). The devices\nused in the study are shown in Figure 1.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.146523"}}
{"text": "Hovsepian et al. [32] Lab, field Lab train data: 21\nparticipants\nLab test data: 26\nparticipants\nField test data: 20\nparticipants", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.147323"}}
{"text": "Online content\nAny methods, additional references, Nature Research reporting \nsummaries, source data, extended data, supplementary informa-\ntion, acknowledgements, peer review information; details of author \ncontributions and competing interests; and statements of data and \ncode availability are available at https://doi.org/10.1038/s41558-\n019-0666-7.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.144367"}}
{"text": " 31. Reichstein, M. et al. Deep learning and process understanding for data-driven \nEarth system science. Nature 566, 195–204 (2019).", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.145119"}}
{"text": "Here, we incorporate a regularized linear regression model that separates signal \nand noise in the extraction of the fingerprint (see Extended Data Fig. 1 for an \nillustration). We use two key climate change metrics as target variables, AGMT  \n(in the year y that corresponds to the day i at which climate change is to be \ndetected) and EEI (as a decadal average in the years before y). These two metrics \nare key indicators of climate change (AGMT18,19; EEI20,21) and serve here as target \nvariables (that is, the one-dimensional test statistic) for climate change detection \nat the daily timescale. The method is conceptually similar to a machine learning \napproach to fingerprint extraction used recently to determine emergence times \nof climate change based on yearly data30. In a first step, the fingerprint of external \nforcing is extracted from forced model simulations such that the p-dimensional \nspatial pattern of daily temperature or humidity is related linearly to one of the \ntarget metrics (denoted here, Ymod) in a regression setting (step (1) in Extended \nData Fig. 1):", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.145929"}}
{"text": "We extract model simulations from the CMIP5 multi-model archive36 for \ntraining. Simulations are extracted for 24 different models from 13 modelling \ncentres that provide daily surface temperature and specific humidity (see \nSupplementary Table 1). The model data consist of in total 14,100 individual \nmodel years available for fingerprint extraction, where three days per month are \nchosen for training: the 5th, 15th and 25th day of each month (corresponding \nto 507,600 days). The 14,100 model years are made up of 45 simulations using \nthe historical (1870–2005; that is, in total 6,120 model years) and Representative \nConcentration Pathway (RCP)8.5 scenario (2006–2100, 4,815 model years), \nand 39 simulations with the RCP2.6 scenario (2006–2100, 3,705 model years). \nWe regrid all daily model data to a common, regular 5° × 5° spatial grid (that is, \ncorresponding to p = 2,592 grid cells or spatial predictors). Next, we subtract the \nseasonal cycle from each day i, separately for each model and for each grid cell p, \nusing a 31-d rolling mean seasonal cycle centred on the respective day. The 31-d \nrolling mean seasonal cycle is estimated from the 1979–2005 reference period. \nThe 1979–2005 reference period is chosen to maximize the overlap between \nmodel simulations with historical forcing (that end in 2005) and observations \n(some of which start only in 1979, see next paragraph), and to ensure an identical \nprocessing and hence comparability between model simulations and observations. \nThe target metrics are obtained similarly for each model. AGMT denotes the \nanomaly of the annual mean spatial average of surface temperatures (in the year \nthat corresponds to day i), and is estimated separately for each model relative to its \n1979–2005 average. The second target metric, EEI, denotes the energy imbalance \nat the top of the atmosphere20, estimated as a decadal average in the decade before \nthe year that corresponds to day i. Net top-of-atmosphere radiation is sensitive \nto drift in CMIP5 models40,41, but the drift is approximately constant in each \nmodel40,41. Therefore, we implement a standard mean drift correction of net top-of-\natmosphere radiation (corresponding to a linear drift in Earth’s energy content40) \nby estimating the mean drift in long-term control simulations in each model. \nSubsequently, we subtract the mean drift from each model’s EEI estimates.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.146606"}}
{"text": "Next, we train the regularized linear model to extract the respective \nfingerprints. For this purpose, an individual statistical model (that is, fingerprint) is \ntrained for each month, because the expected physical response to external forcing \nchanges with the seasonal cycle13 (for example, Fig. 2). To increase the sample size \nfor training of each individual month, samples from the previous and subsequent \nmonth are included in the training step. We implement a standard cross-validation \nscheme to determine the ridge regression parameter (λ) and to extract the final \nfingerprints. Cross-validation is standard practice in statistical learning and \nensures, by splitting the raw dataset into separate partitions, that model fitting and \nmodel validation and selection are performed on different data (to avoid a biased \nperformance evaluation). From a climate science perspective, cross-validation as ", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.147460"}}
{"text": "\nLettersNaTurE ClimaTE CHaNgE\nimplemented here with a ‘leave-one-model-out’ strategy can be seen as an iterative \nperfect model approach. First, in an outer loop, we run 13 simulations (k = 13) \nfor which each individual climate model is iteratively left out as an unseen test \nset. Second, we determine the ridge regression parameter λ in an inner loop by \nk − 1-fold cross-validation. That is, each of the 13 training simulations uses data \nfrom k − 1 = 12 climate models (that is, one model left out iteratively), where each \nmodel is put in a separate fold (‘leave-one-model-out cross-validation’). This \nstep ensures that the fingerprints extrapolate well to an unseen model (that is, \nfingerprints that are robust across the CMIP5 multi-model archive). In cases where \nmodelling centres provide separate variants of a particular model (Supplementary \nTable 1), these model variants are treated as part of the same model and hence put \nin the same fold. We ensure that each model used in the training step receives the \nsame weight by subsampling the number of individual days used for training.  \nThe tuning parameter λ is then selected in the cross-validation as the most \nregularized model within one standard error of the minimum mean squared \nerror on the out-of-fold data. This yields a fingerprint (γ̂k) as a set of regression \ncoefficients. Third, for each of the 13 simulations (that is, iteratively for each test \nset model not used in the inner loop), we predict the respective target metric \nfor any given day i. These independent estimates of the target metric are used to \nestimate prediction errors (that is, the RMSEs discussed in the main text) and \n1870–1950 predictions are used as a reference distribution of the test statistic  \nunder ‘natural variability’. The choice of the 1870–1950 period for the control \ndistribution is conservative, because some of the early-twentieth-century warming \nmay have been externally forced28 in addition to internal variability. Last, the final \nfingerprint (γ̂, shown in Fig. 2), for any given month, is obtained by averaging  \nover the 13 separate fingerprints (γ̂k) of the outer loop.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.148435"}}
{"text": "This set-up is repeated for both climate change target metrics (AGMT and \nEEI), and for different sets of predictors. The different sets of predictors include  \nthe daily spatial pattern of (1) temperature (‘Temp.’); (2) temperature, where  \nthe global mean temperature of each day is removed from each grid cell  \n(‘Temp.’, mean removed); (3) specific humidity, using the mask of an observational \ndataset42 available over land (‘Hum. (land)’); (4) specific humidity over land areas \nwith the global mean removed at each time step (‘Hum. (land), mean removed’); \nand (5) combined mean-removed temperature and land-only humidity  \n(‘Temp. + Hum. (land), mean removed’; that is, (2) and (4) combined). \nSupplementary Table 2 and Supplementary Table 4 provide an overview of \nthe prediction performance for all sets of predictions for AGMT and EEI \nprediction, respectively. As the number of available model runs differs by model \n(Supplementary Table 1), all multi-model quantities from CMIP5 shown in the \npaper (for example, 1850–1950 reference distribution of ‘natural variability’,  \n2.5th to 97.5th percentile range of daily CMIP5 predictions) are weighted such \nthat each model receives equal weight. The prediction error metrics (RMSE) \nare evaluated separately for each model and season, and subsequently averaged. \nTraining of statistical models was conducted in R (version 3.4.3)43 using the \n‘glmnet’ package (version 2.0-16)44.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.149087"}}{"text": "A basic shortcoming in studying the connection between web searches and disease onset is that web searches\nare almost always anonymous when accessible to researchers. This means that linking actual medical infor-\nmation, such as exact disease diagnosis and its date, is limited to indirect inference. An exception to this rule\nis studies where patients consent to contribute their medical records and their online activity for study [14].\nHowever, such studies are necessarily limited by their scale and the diversity of the cohort they recruit. Thus, as\nnoted earlier, most studies collect data from anonymous sources and need to infer the medical status of users.\nThis can be done, for example, using search queries of self-identified users (SIUs).", "meta": {"corpora_id": "acm5.2020-05-22 15:28:34.138230"}}
{"text": "The most widely adopted approach to obtaining clinical indicators of disease is through clinical questionnaires.\nFor example, de Choudhury et al. [2] used questionnaires to assess the level of depression of crowdsourced\nworkers as a basis for using their social media posts to distinguish depressed from non-depressed individuals.\nHere, this approach is adopted to focus on cancer and correlate the score of a clinically validated questionnaire\nto medical symptom searches. In contrast to depression, the incidence of cancer is significantly lower than that\nof depression (creating a recruitment challenge). Moreover, people are more likely to ask for medical symptoms,\nespecially those of a personal nature, on search engines rather than on social media [16]. Therefore, here I used\na targeted advertising campaign and search engine logs as the primary data source.", "meta": {"corpora_id": "acm5.2020-05-22 15:28:34.141491"}}
{"text": "Users were requested to provide their data for the experiment. In the first study, the scores were correlated\nwith searches of users who consented. In the second study, a conversion indicator was given to the advertising\nsystem for users with high scores.", "meta": {"corpora_id": "acm5.2020-05-22 15:28:34.142363"}}
{"text": "The first study was conducted using the Bing ads system and required privileged access to the search system\ndata to obtain past user queries. The second study was conducted using the Google ads system, with no such\nprivileged access to past queries. The latter was done to demonstrate that public health organizations with no\nprivileged access could also utilize these systems. In both studies, the campaign budget was set to $15 per day,\nwhich meant that not all people who issued the relevant queries could be shown the ads. This was done so as to\nallow the ads system in the second study to select relevant participants from all users issuing relevant queries.", "meta": {"corpora_id": "acm5.2020-05-22 15:28:34.143217"}}
{"text": "People were recruited through ads and asked to complete questionnaires. The ads were shown between Decem-\nber 7, 2017, and April 13, 2018, to people in the United States. I extracted all queries made on Bing by users who\ncompleted a questionnaire, consented to contribute their data to the study, and were logged into Bing at the time\nof ad display. The queries were extracted from 3 months before the questionnaire completion and until 20 days\nafter that date. Queries of each person were represented by the following:", "meta": {"corpora_id": "acm5.2020-05-22 15:28:34.144167"}}
{"text": "The SCS was predicted from query data of participants for whom at least 14 days of query data prior to\nquestionnaire completion were available. The independent attributes for prediction included the query terms, as", "meta": {"corpora_id": "acm5.2020-05-22 15:28:34.144835"}}
{"text": "Throughout the experiment, 1,285 questionnaires were started and 681 were completed (53% completion rate).\nIt took an average of 126 seconds to complete the questionnaires. After excluding people who did not consent\nto the participate in the study and people who did not have a query history of at least 14 days, the data from 288\nremaining people (185 lung, 81 colon, and 22 breast) were analyzed.", "meta": {"corpora_id": "acm5.2020-05-22 15:28:34.145628"}}
{"text": "Trained separately, AUCs for the different cancers are 0.74 (colon), 0.56 (lung), and 0.50 (breast). Thus, colon\ncancer is the easiest one to identify, followed by lung cancer. The data do not allow the prediction of breast\ncancer outcome.", "meta": {"corpora_id": "acm5.2020-05-22 15:28:34.146495"}}
{"text": "system have limited access data from these systems. A rank regression model was fit to the impression rate per\nday over the duration of the campaign. The model fit (R2), slope of the impression rate over time, its p-value, and\nthe average impression rate are reported in Table 1. As the table shows, two of the five keywords used to trigger\ncolon cancer ads saw decreased impression rates over time. As expected, their average conversion rates (3.0% and\n4.4%) were well below the overall conversion rate of the campaign. Thus, the system identified these keywords\nas leading many people who did not have suspected cancer to click on the ads. Conversely, the impression rate\nincreased for two keywords related to breast cancer, and the impression rate for these was higher than the first\ntwo keywords. Interestingly, the impression rate for one demographic group (females 18–24 years old) increased\nduring the campaign and had a high conversion rate. It is noted that NICE for breast cancer suggests referring\nwomen in this age group for medical attention only if they have an unexplained breast lump with or without\npain.", "meta": {"corpora_id": "acm5.2020-05-22 15:28:34.147252"}}
{"text": "Understanding the reason people access the questionnaires. People might search for diagnostic information for\ndifferent reasons, including as a way to decide if they should visit a medical doctor, to validate the findings given\nto them by a medical professional, or in lieu of a medical professional. The latter can happen when access to\nthe medical system is restricted either by its availability or by cost (e.g.„ if the patient does not have medical\ninsurance). Thus, to understand who made use of the campaign ads, I modeled the data from 40 countries where\nads were shown at least 150 times.", "meta": {"corpora_id": "acm5.2020-05-22 15:28:34.147886"}}
{"text": "The outcomes of the first study indicate that queries are predictive of SCS for two of the cancer types (colon\nand lung), especially for those people for which the strength of the prediction is highest. It is noted that it is\ndifficult to identify whether the dearth of data or a lack of relevant information in the data was the cause for the\nlow performance in the case of lung and breast cancer.", "meta": {"corpora_id": "acm5.2020-05-22 15:28:34.148455"}}
{"text": "Our study has several limitations. First, although the questionnaires are designed to identify people with\nsuspected cancer, our data does not contain diagnostic information. Obviously, not all people who received a\nhigh SCS will be diagnosed with cancer. NICE questionnaires are designed to balance errors (false positives and\nfalse negatives) in a way that is cost effective (e.g., see Table 3 of Section 4.4 at https://www.nice.org.uk/guidance/\nng12/evidence/appendices-ae-pdf-74333342). However, the fact that a high SCS is not proof of cancer should\nnot detract from the usefulness of the SCS in screening. Indeed, accepted screening tests have relatively modest\ntrue-positive rates: it is estimated that around 6.3% of women in their 50s with an abnormal mammography\nhave invasive breast cancer [11], and studies report that around 4.6% of people with positive fecal occult blood\ntest are verified as having colon cancer [21]. Nevertheless, these screening tests are considered useful and are\nrecommended by health authorities. Future work will focus on a follow-up with people who completed their\nquestionnaire until after diagnosis to measure their detection rates.", "meta": {"corpora_id": "acm5.2020-05-22 15:28:34.148998"}}
{"text": "Studies using Internet data have mostly concentrated on improving population health [22, 33, 35] and showing\nthe feasibility to detect illness [8, 24, 28] but without providing this information in actionable form to people.\nThe current study opens the door for health authorities to (indirectly) use Internet data for screening and to\nprovide medical advice and nudges in a way that empowers users to choose if they would like to utilize it.", "meta": {"corpora_id": "acm5.2020-05-22 15:28:34.149625"}}{"text": "Microsoft Kinect has been successfully used to reconstruct 3D meshes representing humans at real-time\nrates [Du et al. 2019]. However, our preliminary tests with Microsoft Kinect 2.0 showed that the thinness of\nthe catheter makes it hard to detect and track with commodity depth sensors. The shortcomings of infrared\ndepth sensing, in this case, are outside the scope of this work.1 Instead, we devise a new optical marker and\ntracking technique, suitable for augmented reality (AR). We label a portion of the catheter with three distinct\ncolors. One design principle we have observed after consulting our medical collaborators is making minimal\nchanges to the catheter. Our modification does not change the catheter in any way that would make it feel dif-\nferent to operate. We develop an algorithm to detect the color bands and then use them to calculate the position\nof the catheter. This way, even if the tip of the needle is occluded, we still know where it is and are able to\nvisualize it, as long as enough of the colored portion remains visible. We display a virtual catheter overlaid on\nthe real catheter directly in the user’s field of view using a see-through Microsoft HoloLens, which allows the\nsurgeon to better focus on the patient while still having access to the relevant data and provides better depth\nperception and understanding of the operating context. We only need to do a one-time calibration to determine\nthe relation between the HoloLens and the camera. Testing shows that our system achieves high accuracy and\nlow latency. We also show the usefulness of our system in a realistic surgical environment on a cadaveric head.\nAn example of the view directly observed by the surgeon is shown in Figure 1.", "meta": {"corpora_id": "acm6.2020-05-22 15:28:34.137882"}}
{"text": "digital life, people are increasingly accumulating long-term tracking data, ranging from short bursts of tracking\nto multiyear records of activity or health data. We are just starting to understand how this introduces new\nopportunities, as well as challenges [3]. Building upon a workshop conducted at CHI 2018 [4], we subsequently\nidentify key features of long-term tracking, present a model for long-term tracking, and suggest a research\nagenda.", "meta": {"corpora_id": "acm2.2020-05-22 15:28:34.139548"}}
{"text": "Incompleteness of data: Although trackers may be used every day in the short term, this may change in the\nlong term, as people will undoubtedly stop tracking for a few hours or days, or for weeks, months, or years,\nintentionally or accidentally, resulting in incomplete data [8]. Gaps in the data, which in the short term are\noften considered an exception, are therefore the rule and can even carry information of their own.", "meta": {"corpora_id": "acm2.2020-05-22 15:28:34.140204"}}
{"text": "Implicit tracking with secondary sources: With the increasing digitization of daily life, we collect tremen-\ndous amounts of data about ourselves in secondary sources such as social networks, chats, workplace pro-\nductivity software, and online services. Although such data may be unstructured, heterogeneous, and frag-\nmentary, the longitudinal coverage makes it a particularly interesting supplementary source to understand\ncharacteristics such as context and connectedness over the long term.", "meta": {"corpora_id": "acm2.2020-05-22 15:28:34.141531"}}
{"text": "Subjectivity of data: Data is not just an objective measurement. It may be amenable to multiple interpreta-\ntions [8]. It may also tell a story of a person’s context, situation, setting, and memories at the time it was\ncollected [5], making it a memento of experiences such as happy changes in life, stressful times, or diseases.", "meta": {"corpora_id": "acm2.2020-05-22 15:28:34.142343"}}
{"text": "Secondary user: Parents of small children, medical experts, formal or informal caregivers, and other sec-\nondary users may have a legitimate reason to access the tracked data but are not involved in the challenges\nof the tracking itself. In the long term, this raises additional issues such as changing data ownership and\nresponsibility from the parents to an adolescent.", "meta": {"corpora_id": "acm2.2020-05-22 15:28:34.143171"}}
{"text": "Ethical, legal, and social implications: With long-term tracking, data becomes a virtual representation of\nthe person, capable of surfacing trends in the person’s health and relationships. Long-term data therefore\nbecomes a highly personal and lifelong asset that requires even more attention to questions about data\nprotection, access rights, security, privacy, or data ownership.", "meta": {"corpora_id": "acm2.2020-05-22 15:28:34.143816"}}
{"text": "As existing models of short-term tracking such as those of Li et al. [2] do not account for these aspects, we\npropose the long-term tracking feedback loops in Figure 1. Here, the user is both a producer of data and consumer\nof services, with potentially conflicting demands. As a producer of data, users may decide to trade off data quality\nbecause they want to reduce the effort they need to make for data collection. This may result in less data or lower-\nquality data. As a consumer of the services delivered by applications, there is a need for enough high-quality data.\nConsequently, we distinguish two different types of tracking: purposeful and incidental.", "meta": {"corpora_id": "acm2.2020-05-22 15:28:34.144462"}}
{"text": "Purposeful tracking is driven by a user’s need for a certain form of support or service. The application delivering\nthis therefore requires certain data, which must come from relevant tracking devices and sources. Ultimately,", "meta": {"corpora_id": "acm2.2020-05-22 15:28:34.145263"}}
{"text": "of data. These two loops are affected by a potential conflict between minimizing the demands upon the user and the need", "meta": {"corpora_id": "acm2.2020-05-22 15:28:34.146061"}}
{"text": "the purpose drives the tracking behavior required of the user. Changes in the users’ tracking goals and their\ncommitment to tracking have often compromised adherence [8], resulting in incomplete or missing data.", "meta": {"corpora_id": "acm2.2020-05-22 15:28:34.146911"}}
{"text": "By contrast, in incidental tracking, users not have a specific need. Tracking happens as a side effect of routinely\nusing devices or feeding secondary sources. Type, amount, and quality of data are determined by a user’s tracking\nroutine, not by potential future needs. These potential limitations of the data may in turn limit the types of\nservices that an application can offer.", "meta": {"corpora_id": "acm2.2020-05-22 15:28:34.148559"}}
{"text": "Long-term and short-term tracking share many properties and challenges. However, for many reasons, research\nso far has primarily focused on people’s short-term goals and needs. We suggest that the research community\nis missing an understanding of the specific opportunities of long-term tracking. This requires understanding\nhow long-term tracking is different from short-term tracking that we identified previously. It is more incidental\nrather than purposeful, and it is less likely to be for a specific, proximal goal. It may involve repurposing data,\ngiving it value the user did not anticipate. It can change as people’s long-term goals and understanding of them\nevolve. The data and its quality are defined by the user’s willingness to track, and applications must be satisfied\nwith whatever data is there. Therefore the main challenge is to maximize the value of existing data. This calls\nfor future research, including, but not limited, to the following:", "meta": {"corpora_id": "acm2.2020-05-22 15:28:34.149113"}}
{"text": "Systems issues for implicit tracking: This requires infrastructure, specifically designed to ensure security,\nuser control of privacy, and data provenance and methods to manage and analyze diverse data sources with\nheterogeneous data.", "meta": {"corpora_id": "acm2.2020-05-22 15:28:34.149834"}}
{"text": "Interfaces to visualize long-term data: Visualizing and exploring one’s own data is the initial step to self-\nunderstanding. However, short-term approaches do not scale well when it comes to unstructured, heteroge-\nneous, and large long-term data. New visualization techniques are needed to make long-term data accessible\nand support sense making by the layperson.", "meta": {"corpora_id": "acm2.2020-05-22 15:28:34.150613"}}
{"text": "Quantifying migration activity from filtered WSR data. From clutter- and \nprecipitation-free data, we calculated the height profiles of migration intensity, \nspeed and direction using the lowest elevation scans (0.5–4.5°), at distances \nbetween 5 and 37.5 km from the radar. We determined the migration intensity \nfrom reflectivity (η (cm2 km−3)) and migrant flight direction (that is, track) and \nground speed from the radial velocity from 100 to 3,000 m above ground level \nwithin 100 m altitudinal bins43–46. When necessary, we de-aliased the radial velocity \nmeasures using the WSRLIB package47,48. To limit the influence of migratory \ninsects, we excluded altitudinal bins with velocity azimuth displays with a root \nmean squared error less than one, and we removed samples with a root mean \nsquared error greater than ten to limit the poor fits49,50. We further restricted \nsampling nights to measures with seasonally appropriate flight directions, \nallowing only samples with a northward component in the spring and a southward \ncomponent in the autumn (between 90 and 270°, depending on the season).", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.144199"}}
{"text": "Quantifying change in the surface air temperature. To relate interannual \nvariation in the peak migration with climate, we extracted data on the diurnal air \ntemperatures (°C) at 2 m above ground from the NCEP North American Regional \nReanalysis54 for the same dates for which WSR data were analysed. We extracted \ndiurnal temperature measures from the radar coverage area (37.5 km from the \nradar) and averaged the daily measures within each year, which resulted in a \nseasonal time series per WSR station. To quantify the seasonal change in surface \ntemperature, we averaged the temperatures with season (spring or autumn) \nand used a ridge-regression linear model with WSR ID as a fixed effect and an \ninteraction between WSR ID and year. This yielded site-specific coefficients of \nchange for each WSR station. We used a penalty of 0.00001.", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.144925"}}
{"text": "Data availability\nThe datasets generated during and/or analysed during the current study are \navailable at https://doi.org/10.6084/m9.figshare.10062239.v1.", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.145694"}}
{"text": "WSR-88D data. Bull. Am. Meteorol. Soc. 74, 645–653 (1993).\n 36. Rosenberg, K. V. et al. Decline of the North American avifauna. Science 366, ", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.146555"}}
{"text": "Author contributions\nAll the authors worked to conceive and design this study. K.G.H., F.A.L., D.S. and \nA.F. drafted the manuscript. T.-Y.L., K.W., G.B., S.M., K.G.H. and D.S. designed the \nradar algorithms, and processed and summarized the radar data. K.G.H. generated the \nfigures and D.S., W.M.H. and K.G.H. designed the analyses. All the authors provided \neditorial advice, approved the final version of this manuscript and are in agreement to be \naccountable for all aspects of the work.", "meta": {"corpora_id": "nature3.2020-05-22 15:28:34.147304"}}
{"text": "Results\nOur empirical analysis of the drivers of energy use patterns and car-\nbonization is based on data on population, gross domestic prod-\nuct (GDP), fuel-specific primary energy and CO2 emissions from \nthe International Energy Agency (IEA)15. This includes data for 24 \ncountries in SSA (another 25 are aggregated into Other Africa) for \nthe period 1971–2015. As the main focus of our analysis is on coun-\ntries that are building up their energy systems, we exclude South \nAfrica from the SSA aggregate.", "meta": {"corpora_id": "nature4.2020-05-22 15:28:34.148701"}}
{"text": "We then project the development of power sector emissions in \nthe near future, on the basis of the Platts database16, which provides \ncountry-level information on power plant capacities that already \nexist, are under construction or are in the planning process. Finally, \nwe compare these results with projected emissions in the Shared \nSocioeconomic Pathways (SSPs)17 that have been derived from \nintegrated assessment model studies. All data and methods are \ndescribed in more detail in the Supplementary Information.", "meta": {"corpora_id": "nature4.2020-05-22 15:28:34.149372"}}
{"text": "Economic development in sub-Saharan Africa has increased carbon emissions and will continue to do so. However, changes \nin emissions in the past few decades and their underlying drivers are not well understood. Here we use a Kaya decomposition \nto show that rising carbon intensity has played an increasingly important role in emission growth in sub-Saharan Africa since \n2005. These changes have mainly been driven by the increasing use of oil, especially in the transportation sector. Combining \ninvestment data in the power sector with economic and population projections, we find that investments in new coal-fired \ncapacity may become a major driver of future carbonization. Our results highlight the importance of making low-carbon tech-\nnologies available and financially attractive to sub-Saharan African countries to avoid a lock-in of emission-intensive energy \nuse patterns.", "meta": {"corpora_id": "nature4.2020-05-22 15:28:34.149980"}}
{"text": "Online content\nAny methods, additional references, Nature Research reporting \nsummaries, source data, extended data, supplementary informa-\ntion, acknowledgements, peer review information; details of author \ncontributions and competing interests; and statements of data and \ncode availability are available at https://doi.org/10.1038/s41558-\n019-0649-8.", "meta": {"corpora_id": "nature4.2020-05-22 15:28:34.150618"}}
{"text": "Projected data. The projected data for 2025 result from combining three data \nsources by the IEA15, Platts16 and Shearer et al., with a set of assumptions. A \nsummary diagram of the methodology used to compute the Kaya factors is \nprovided in Supplementary Fig. 1. For instance, GDP and population data over \n2005–2015 were taken from the IEA15 and were linearly extrapolated to 2025.", "meta": {"corpora_id": "nature4.2020-05-22 15:28:34.151264"}}
{"text": "The associated CO2 emissions were estimated using carbon intensities from \nthe previous periods for all fossil fuels. More specifically, the carbon intensities of \ncoal, oil and gas power plants are about 950, 850 and 400 g CO2 kWh−1, respectively. \nThese were computed using IEA data15 and were relatively constant between \n2000 and 2015. They are in good agreement with the 2006 IPCC Guidelines for \nNational Greenhouse Gas Inventories35. In other words, CO2 emissions in 2025 are \ncalculated using the projected energy consumption in the power sector and other \nsectors multiplied by the relevant CO2 emission factors.", "meta": {"corpora_id": "nature4.2020-05-22 15:28:34.151802"}}
{"text": "On the basis of these data and assumptions, we generated two polar scenarios: \n30% and All. In 30%, we assumed that only 30% of the pre-construction coal power \nplant projects will be successful while all shelved projects will not. This assumption \nreflects historical rates as indicated in Shearer et al.22 (Africa and the Middle East: \n27%, World: 34%; see Table 2 in the report). We also considered all coal power \nplants to run at 30% of their capacities, which is also in line with historical data \n(Supplementary Fig. 3). In All, we assumed that all planned coal plants (pre-\nconstruction and shelved) will successfully come online by 2025 and that all coal \nplants have a load factor of 48% (Supplementary Fig. 3). In both scenarios we also \nassumed that all coal power plants existing in the previous period are still operating \nand that all plants in construction will be fully operating in 2025. The assumptions \nunderlying these two scenarios cover a broad range of highly disputed values. The \nfuture will probably lie somewhere between the two scenarios. We emphasize, \nhowever, that our results indicate an influence on short-term emission growth in \ngeneral; that is, they do not necessarily depend on the year 2025 specifically but \nrather sometime around this year.", "meta": {"corpora_id": "nature4.2020-05-22 15:28:34.152354"}}
{"text": "is better than with wrist-worn optical sensors [3, 4]. We wanted a heart rate monitor that supported BLE so the\ndata could stream to the Amulet. At the time, there were two popular BLE-capable, chest-mounted heart rate\nmonitors available on the market: the Zephyr HXM and the Polar H7. Both are capable of streaming data and\nfollow the standard Heart Rate Profile protocol specifications.5 Both devices transmit data to the Amulet using\nBLE at 1 Hz. Each data packet consists of one heart rate value and one or more R-R interval values.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.148084"}}
{"text": "We conducted a preliminary test to compare these heart rate monitors to a popular clinical ECG device—the\nBiopac MP150 [7]. We first measured participants with the Zephyr and the Biopac, and then with the Polar H7 and\nthe Biopac. We then divided the data collected from each device pair into 30-second windows and computed some\nbasic heart rate and R-R interval features. We used a Pearson correlation to compare the feature values between\nthe two devices; the results are shown in Table 2. On inspecting the r -coefficients from the two comparisons, we\nobserve that the features computed from the Biopac were more strongly correlated with the Polar H7 than with\nthe Zephyr HXM. Given the better performance of the Polar H7, we used it for the study.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.148621"}}
{"text": "3.1.2 The Amulet Wearable Platform. The Amulet is an open source hardware and software platform for writ-\ning energy- and memory-efficient sensing applications [9, 31]. The Amulet has several on-board sensors and pe-\nripherals, including a three-axis accelerometer, light sensor, ambient air temperature sensor, buttons, capacitive\ntouch slider, micro-SD cards, LEDs, and a low-power display. We used the Amulet to act as a data hub to receive\nthe heart rate data using BLE, to record accelerometer data and to prompt Ecological Momentary Assessment\n(EMA) questions to the participant. The Amulet stored all of the sensor data and EMA responses on its internal\nmicro-SD card. The rationale behind using the Amulet for in-the-wild data collection and storage instead of a\nsmartphone was that a wearable would always be on the body of the participant, thus reducing the chances of\ndata loss when the phone was not in range of the person. In addition, the Amulet had additional physical buttons\nthat we were able to map for specific tasks, as described in Section 3.4. Finally, a wearable like Amulet can collect\ndata about the participant’s stress and physical activity even when the smartphone is on the table, in another\nroom, or being used by someone else.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.149133"}}
{"text": "We first described the details of the study to the participants, and they consented to participate. Next, partic-\nipants put on both the heart rate sensor and the Amulet (which was used solely to to collect the data from the\nheart rate sensor). Once the devices were in place, we began data collection. Each device collected data through-\nout the lab experiment (about 40 minutes). Participants were asked to not move, remove, or interact with the\nsensors in any way.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.149731"}}
{"text": "At the end of the initial baseline rest period and after each stressor, we asked the participant to verbally rate\nhis or her stress level on a scale from 1 to 5; this was the stress perceived by the user. As the ground truth, we\nlabeled each minute of data collected in the lab as stressed (class = 1) or not stressed (class = 0), based on whether\nthe participant was experiencing a stressor stimulus within that minute.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.150294"}}
{"text": "For the field study, participants were asked to wear the sensing system for 3 days (at least 8 hours per day) while\ncarrying out their everyday activities. To ensure that the battery of the devices did not drain before the end of\nthe day, we duty cycled the sensing system to record the physiological data for 1 minute every 3 minutes. In\naddition to recording the physiological data, the Amulet prompted the participants to answer EMA questions\nonce every 30 minutes. The participants also had the option to proactively report a “stressful” event by clicking\non a dedicated “event mark” button. When the participant clicked on this button, the Amulet would record the\ntime as a stressful event, and (in a fraction of such cases) the Amulet would randomly prompt the participant to\ncomplete an EMA questionnaire. If an EMA prompt was triggered due to an event mark, the system would not\ntrigger another EMA again in that 30-minute period, to prevent participant overload.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.150916"}}
{"text": "While we attempted to collect heart rate variability (HRV), EMA, and accelerometer data for 27 participants in\nthe lab and field, there were problems that led to loss of some data. We ran into a problem with corrupt SD cards,\nwhich led to partial field data loss for 2 participants. We also lost the complete lab data for 1 participant, due to\nthe same SD card problem. Eventually, we ended up with 26 participants for whom we had heart rate data both\nin the lab and the field.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.151477"}}
{"text": "Figure 3 provides a summary of the data collected; we quantify the amount of lab, field, and EMA data collected\nfrom each participant.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.151947"}}
{"text": "We now discuss our methods for processing the data, which includes data cleaning, normalization, and feature\ncomputation and selection.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.152329"}}
{"text": "Once the participants returned the devices to us, we extracted the data from those devices. The Amulet logged\nthe heart rate, R-R interval, accelerometer, EMA, and event mark data to files on the built-in micro-SD memory\ncard. The Amulet encrypted these files as they were written to ensure the data was not compromised in case\na participant lost the Amulet and/or the micro-SD card. We decrypted the data using a Ruby script that also\ngenerates a “.csv” data file for each participant.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.152714"}}
{"text": "We began with preliminary data cleaning to filter out invalid data points. In this step, we were not trying to handle\noutliers (which may or may not be valid readings) but wanted to remove obviously erroneous data readings. This", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.153411"}}
{"text": "Fig. 3. Summary of the data collected during the field component of the study.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.153880"}}
{"text": "If the heart rate value was outside a predetermined range, it was considered as noise and removed. We dropped\nboth the heart rate value and any R-R interval values received in that second. Based on previous research con-\nducted to find the maximum human heart rate [21, 51], we set our upper bound to 220 bpm. To determine the\nlower bound, we inspected heart rate data of all participants (visually) to find any noticeable value that would\nseem invalid. The resulting range [30:220] bpm is very conservative; we are confident that any data point outside\nthis range is invalid.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.154315"}}
{"text": "As reported in Section 3.5, we received 536 event marks when the participants just clicked on a dedicated\nbutton on the Amulet to mark a stressful event. Although participants found it to be easy to mark an event as\nstressful, we fear that it might have been too easy. During the exit interview (i.e., when the participants came\nback to return the devices after the field setting), some of the participants complained about “accidental clicks” on\nthe event mark button, and without an option of undoing the mistake, the Amulet marked the time as a stressful\nevent. We investigated further to determine the extent of the problem. While answering the self-reports, the\nparticipants had to choose their stress level on a scale from 1 to 5. For each participant, we calculated the mean\nscore to all reports they answered. If a participant’s self-report value was higher than his or her mean, then\nwe labeled that instance as a stressed instance (class = 1); otherwise, we labeled it as not stressed (class = 0).\nNow, according to the study design, the participants might randomly receive a prompt to complete a self-report\nafter they click on the event mark button. Of the 536 instances we received, the participants were prompted to\ncomplete the self-report 300 times. Of these 300 instances, the participants chose a stress value greater than their\nindividual mean score only 112 times. This suggests that more than 60% of the time, the participants might have\nclicked on the event mark button by mistake. Without any means to validate if the remaining 236 instances are\ngenuine, we decided not to use the data collected by event marks for training or evaluating our model. We intend\nto fix this problem in future studies.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.154645"}}
{"text": "Data from reanalyses and observations. To assess short-term detection \nbeyond climate models, we project three daily reanalysis datasets, three monthly \nobservational datasets and one daily observational dataset onto γ̂ to estimate our \ntest statistic. As outlined above, the projection is performed separately for both \ntarget metrics (AGMT and EEI), for each set of predictors and for each day i using \nthe fingerprint of the respective month.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.149873"}}
{"text": "The reanalyses include ERA-Interim45, the NCEP/NCAR Reanalysis 146 and  \nthe 20th Century Reanalysis version 2c47. Spatial coverage of the reanalysis datasets \nis global, and with daily temporal coverage. The temporal coverage spans all \ndatasets with a combined 168-yr period (that is, 1979–2018, 1948–2018,  \nand 1851–2012, respectively).", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.150485"}}
{"text": "In addition, we use monthly gridded temperature and specific humidity \nobservations. Three monthly temperature datasets are available with near-global \nspatial coverage and in monthly temporal resolution. These datasets include: the \nBerkeley Earth Surface Temperatures48 (BEST), the Cowtan and Way temperature \nreconstruction49 (CW14) based on HadCRUT450 and the National Aeronautics and \nSpace Administration’s GISS Surface Temperature Analysis51 (GISTEMP, version \n3). All three datasets have global (CW14, 1850–2018) and near-global coverage \n(>99.4% in space after regridding to a 5° × 5° regular grid, BEST, 1956–2018,  \nand GISTEMP v3, 1957–2018) obtained through a statistical reconstruction to \ninfill observational gaps48,49,51 from station-based land temperatures blended  \nwith sea surface temperature measurements. Note that sea surface temperatures ", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.150947"}}
{"text": "show slightly less warming than air temperatures above the sea52 (for example,  \na difference of around 0.031 °C in the 2009–2013 period relative to 1961–199052). \nThis might imply very small differences compared with natural variability \nestimates from CMIP5 that are based on air temperatures and that the increase in \nthe test statistic derived from blended observations is slightly too conservative.  \nThe fact that results based on observations and reanalyses are so similar suggests \nthat the effect is small. For specific humidity, gridded observations are available \nonly for land areas27. We use the Met Office HadISDH gridded global land surface \nhumidity dataset42 (spanning 1973–2017), which features a reasonable coverage \nof global land areas. We mask the dataset to all grid cells that have a coverage of at \nleast 95% in time. This yields a land humidity dataset with a maximum of 3% gaps \nin space at any particular time step, which still samples all major land regions of \nthe globe (Fig. 2e), and where in fact 519 out of 540 time steps (96.1%) have less \nthan 1.5% gaps in space (after regridding to a 5° × 5° regular grid; which yields \n520 grid cells with data). This mask is used also for fingerprint extraction where \n‘land humidity’ is included as a predictor. The small number of remaining gaps in \nthe temperature and humidity gridded observations were filled with zeros (that is, \ncorresponding to the monthly mean of the 1979–2005 reference period).", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.151479"}}
{"text": "The processing of reanalysis and observational datasets follows exactly the \nprocessing of CMIP5 models. That is, all data are regridded to a regular 5° × 5° \ngrid and the seasonal cycle (31-day rolling mean for daily reanalysis datasets, and \nmonthly mean seasonal cycle for monthly gridded observations), estimated from \nthe 1979–2005 period, is subtracted from each grid cell before further analysis.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.152133"}}
{"text": "In addition to monthly observations, we construct a daily observational \ndataset that spans 1981–2018 by combining a daily sea surface temperature dataset \n(OISST-AVHRR53) with daily observational land data (Berkeley Earth Gridded \nDaily Data48). Daily observations should be considered as experimental and are \nshown only for illustration purposes, because Berkeley Earth daily land data are \nstill in development. All details regarding dataset generation and discussion of \npotential caveats are described in Supplementary Text 1.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.152769"}}
{"text": "Robustness analysis of AGMT detection statements. We assess the robustness \nof detection statements in Supplementary Text 3. This includes a detection and \nemergence analysis equivalent to Figs. 3 and 4 but for all datasets individually \n(Supplementary Text 3.1), an assessment of the robustness of detection statements \nagainst individual CMIP5 models used to construct natural variability estimates \n(Supplementary Text 3.2) and an analysis of the influence of low-frequency \nvariability on the distribution of the test statistic (Supplementary Text 3.3).  \nIn addition, we show how detection results depend on the timescale of analysis  \nin Extended Data Fig. 3.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.153538"}}
{"text": "Data availability\nAll original CMIP5 data, reanalyses and observations used in this study are  \npublicly available under the following URLs. CMIP5 model data: https://esgf-node. \nllnl.gov/projects/cmip5/; reanalysis: ERA-Interim (https://www.ecmwf.int/en/\nforecasts/datasets/reanalysis-datasets/era-interim), NCEP/NCAR Reanalysis \n1 (https://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html), \nNCEP/NCAR Reanalysis 2 (https://www.esrl.noaa.gov/psd/data/gridded/data.\nncep.reanalysis2.html), Twentieth Century Reanalysis (https://www.esrl.noaa.\ngov/psd/data/20thC_Rean/); observations (monthly): GISTEMP temperature \ndataset, version 3 (https://data.giss.nasa.gov/gistemp/), Cowtan and Way (2014) \ntemperature dataset, version 2 (https://www-users.york.ac.uk/~kdc3/papers/\ncoverage2013/series.html), Berkeley Earth Monthly Land+Ocean temperature \ndataset (http://berkeleyearth.org/data/), Met Office gridded land surface humidity \ndataset (HadISDH), version 4.0.0.2017f (https://www.metoffice.gov.uk/hadobs/\nhadisdh/); observations (daily): Berkeley Earth Daily Land temperature dataset \n(Experimental, http://berkeleyearth.org/data/), NOAA Optimum Interpolation \nSea Surface Temperature (OISST), AVHRR-Only (https://www.ncdc.noaa.gov/\noisst). All intermediate and derived data from these products (extracted CMIP5 \nfingerprints and daily/monthly time series of the test statistic (that is, obtained \nby projecting CMIP5 models, reanalyses and observations individually onto \nthe fingerprints)) are available at https://data.iac.ethz.ch/Sippel_et_al_2019_\nDailyDetection/.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.153986"}}
{"text": "Code availability\nAll computer code to reproduce the main results and all figures and Extended Data \nfigures is available at https://data.iac.ethz.ch/Sippel_et_al_2019_DailyDetection/.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.154443"}}
{"text": " 45. Dee, D. P. et al. The ERA-Interim reanalysis: configuration and performance \nof the data assimilation system. Q. J. R. Meteorol. Soc. 137, 553–597 (2011).", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.154753"}}
{"text": " 50. Morice, C. P., Kennedy, J. J., Rayner, N. A. & Jones, P. D. Quantifying \nuncertainties in global and regional temperature change using an ensemble of \nobservational estimates: the HadCRUT4 data set. J. Geophys. Res. 117, \nD08101 (2012).", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.155050"}}
{"text": "acknowledgements\nWe thank A. Merrifield, I. Medhaug, G. Obozinski and H. Lange for comments,  \nand we thank U. Beyerle and J. Sedlàček for the preparation and maintenance of  \nCMIP5 data. We acknowledge funding received from the Swiss Data Science Centre \nwithin the project ‘Data Science-informed attribution of changes in the Hydrological \ncycle’ (DASH, ID C17-01). We thank the observers, creators, maintainers and providers \nof all datasets. Support for the Twentieth Century Reanalysis Project version 2c \ndataset is provided by the US Department of Energy, Office of Science Biological \nand Environmental Research (BER), and by the National Oceanic and Atmospheric \nAdministration Climate Program Office. NCEP Reanalysis and NCEP Reanalysis 2 \ndata were provided by the NOAA/OAR/ESRL PSD, Boulder, CO, USA. We thank the \nWorld Climate Research Programme’s Working Group on Coupled Modelling, which \nis responsible for CMIP, and we thank the climate modelling groups for producing and \nmaking available their model output. For CMIP, the US Department of Energy’s Program \nfor Climate Model Diagnosis and Intercomparison provides coordinating support \nand led the development of software infrastructure in partnership with the Global \nOrganization for Earth System Science Portals.", "meta": {"corpora_id": "nature1.2020-05-22 15:28:34.155398"}}
{"text": "Using physiological signals to detect stress has its own drawbacks. The physiological response to mental stress\nis similar to that exhibited due to physical activity and strain. Hence, it is imperative to be able to distinguish\nwhether an observed physiological arousal was due to mental stress or just physical activity. To this end, we\ncollect accelerometer data from the Amulet along with the physiological readings. We use an activity-detection\nalgorithm developed for the Amulet in a study with 14 undergraduate participants [8].", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.155043"}}
{"text": "The activity-detection algorithm uses the accelerometer data and, for every second, infers one of six different\nactivities: lying down, standing, sitting, walking, brisk walking, or running. For every minute in the field data,\nwe determined the dominant activity level—low, medium, or high—based on the activity for each second. If the\nactivity level for a 1-minute window was low (lying down, sitting, or standing), then we included that window\nin our analyses.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.155463"}}
{"text": "We next use the data remaining after the previous steps to compute features to quantify HRV. We split the data\ninto 1-minute intervals and compute a set of features for each interval. However, before we compute some fea-\ntures for further analyses, it is critical that we (1) handle the effect of outliers in the data and (2) remove any\nparticipant-specific effects on the data, so as to create a generalized model, without any participant dependency.\nThese issues would significantly impact the computed features and eventually the accuracy of the results ob-\ntained. We thus look at each in more detail to understand how the results change with different methods for\nhandling outliers and normalization. All of the previous works we reviewed seem to have just selected some\nmethod for handling outliers (if any) and normalization without taking into account the effect of their choice on\nthe outcome of the metrics under study.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.156155"}}
{"text": "4.5.1 Outliers. While dealing with outliers in data, the common approaches are (1) leave them in the data,\n(2) reduce the effect the outliers might have, or (3) remove them completely. In our work, we look at each of\nthese approaches and their effect on model training and evaluation. For the first approach, we do nothing to\nthe data (i.e., leave it as is). In the second approach, we use winsorization7 to reduce the effect of outliers on the", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.156470"}}
{"text": "dataset [56]. This approach was also used by some of the previous works, such as cStress [32] and the work by\nGjoreski et al. [26]. For the third approach, we simply remove (trim) data points that we deem as outliers.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.156777"}}
{"text": "We define outlier as a point that lies beyond a certain threshold above or below the median of the data. For\nour purposes, we set the threshold at three times the median absolute deviation (MAD) within that participant’s\ndata. This choice ensures that we considered only the extreme values as outliers and more than 99% of the data\nis unaltered. Having defined outlier, we establish the upper and lower bounds as follows:", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.157111"}}
{"text": "4.5.2 Normalization. Normalization is important to remove participant-specific effects on the data so as to\nmake the model generalizable to any participant. We tried two different methods for data normalization. With\nphysiological data (e.g., heart rate, Galvanic Skin Response (GSR), skin temperature), each participant has a\ndifferent natural range. Hence, the first normalization method we try is minmax normalization, which simply\ntransforms the values into the range [0, 1]. Given a vector x = (x1,x2, . . . ,xn ), the minmax normalized value for\nthe ith element in x is given by", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.157423"}}
{"text": "4.5.3 Feature Computation. We grouped the normalized data into 1-minute windows. Given the short dura-\ntion of our lab experiments, we wanted to select the shortest possible window size. Esco and Flatt [20] demon-\nstrated that, as compared to 10- or 30-second windows, the features computed in the 60-second window size\nhad the highest agreement with the conventional 5-minute window size. Furthermore, the 1-minute window\nhas been common in physiological monitoring [29, 32, 42].", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.157665"}}
{"text": "For the HRV data, we selected only the time-domain features for our work, as shown in Table 5. All of these\ntime-domain features have been shown to be effective in predicting stressful periods by other researchers [32].", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.158044"}}
{"text": "Unlike HF, which represents parasympathetic activity, LF is less clear. Although some researchers believe\nthat LF represents sympathetic activity, others suggest that it is a mix of both sympathetic and parasympathetic\nactivities [6]. Furthermore, the rationale behind using the LF:HF ratio is that since HF represents parasympathetic\nactivity, a lower HF will increase the ratio, suggesting more stress; however, since the role of LF is not really clear,\nlooking at the ratio might be misleading as well [6]. In addition, for computing LF, we need a window size of\nat least 2 minutes, which would reduce our data size by half. Furthermore, earlier work like cStress found that\ncompared to other time-domain features, and HF, the feature importance of LF and LF:HF is extremely low [32].\nHence we decided to leave out LF and LF:HF features from our work, thus not requiring us to calculate any\nfrequency-domain features.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.158304"}}
{"text": "In this section, we evaluate our approach. We begin by determining whether we were able to capture a signifi-\ncance difference between the resting and stress-induced periods of the lab component, followed by building and\nevaluating machine-learning models from the lab dataset, and finally using the models built in the lab to infer\nstress/not-stress in the field.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.158536"}}
{"text": "We first determined whether we could distinguish between resting state and stressful states in the lab data.\nTo this end, we use features computed from the first 10 minutes of the initial rest period and compare them\nindividually to the features computed from the math test, book test, and cold test, respectively. We used Welch’s t-\ntest of unequal variances to determine which features showed any statistically significant differences between the\nresting baseline period and each of the stress-induction periods. As described earlier, we followed three ways of\nhandling outliers and two ways for data normalization, leading to a total of six combinations, as shown in Table 4.\nAcross all six combinations, we observed the maximum number of features showing significant differences in\nthe trim_zscore combination, and for the sake of space, we report results only for that one combination (i.e.,\ntrimmed outliers and z-score normalization).", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.158908"}}
{"text": "Having determined that the features computed from heart rate data (as measured by a readily available, com-\nmercial, off-the-shelf, heart rate monitor (the Polar H7)) showed significant differences between rest and stress-\ninduced periods, we next used these features (mentioned in Table 5) to build machine-learning models designed\nto infer whether the person is stressed or not stressed. Further, during a stressful period, we look at the feasibility\nof differentiating among the three types of stressors: math, book, and cold tests.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.159186"}}
{"text": "We evaluated each classifier for all the six dataset combinations as mentioned in Table 4 and report three\nmetrics: precision, the fraction of those instances labeled “positive” that actually are positive instances; recall, the\nfraction of positive instances labeled correctly as positive; and F1-score, the harmonic mean between precision\nand recall. The F1-score is a popular metric in classification problems with one primary class of interest.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.159418"}}
{"text": "While we did the training and evaluation for each of the six combinations of outlier handling and normalization\nmethods, we observed that outlier_minmax and outlier_zscore consistently performed the worst (on all three\nmetrics: precision, recall, and F1-score) across all six combinations (which was expected, since we did not handle\noutliers in these two combinations, and leaving them as is in the data could have introduced a bias). Hence, we\ndo not report results from those two combinations and show comparisons among the other four options.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.159670"}}
{"text": "In Table 9, we observe that the best result was achieved by SVM on the trim_zscore combination—that is,\ntrim outliers, then z-score normalization. It is interesting to note that while RF produced a consistent F1-score of\napproximately 0.73 (with varying precision and recall) across the different datasets, SVM showed a wide variation\nof F1-scores: from 0.66 to 0.81.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.159901"}}
{"text": "8It is important to note that we are not suggesting that the Polar H7 is better than a high-quality or clinical-grade ECG sensor. Instead, we\nbelieve with the right data processing pipeline, commodity devices might suffice for the task of stress detection.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.160164"}}
{"text": "In this section, we evaluate the models developed in the lab component of the study for stress detection in the\nfield setting. As described previously, in the field component of the study, we asked the participants to wear\nthe devices in their natural environment and prompted several EMA questions to gather the ground truth. One\nof those questions was “Rate your stress level over the last 10 minutes.” We specifically asked about the last\n10 minutes rather than a generic “How stressed do you feel” to reduce the errors in self-reported data due to\nparticipant recall by limiting them to think about only the last 10 minutes.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.160398"}}
{"text": "To generate the field dataset, we consider the physiological data collected in the 10 minutes leading to the\nself-report answer time. Our sampling strategy was to collect data for 1 minute every 3 minutes (i.e., sample\ncontinuously for 1 minute, then pause for 2 minutes). We took this approach to conserve battery life on the\nAmulet wrist devices. Hence, according to our sampling strategy, we recorded three (sometimes four) 60-second\nwindows corresponding to the 10-minute window prior to each self-report. We computed the features for each\n60-second window and labeled it as stressed (i.e., 1) or not stressed (i.e., 0) based on the response to the 5-point\nLikert scale report from the participant. To binarize the 5-point scale to a simple 1 or 0, for each participant we\ncalculated the median score across all self-reports by that participant; for each report, if the score was greater\nthan the median score, we labeled it as 1 (i.e., stressed), and otherwise, we labeled it as 0 (i.e., not stressed).", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.160632"}}
{"text": "It is important to note that we evaluated the classification results (1) for the entire field data and (2) by removing\nthe activity confounds—that is, by only considering those 60-second windows where the inferred activity level\nwas low.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.160861"}}
{"text": "To infer stress in the field dataset, we used the models previously generated in the lab setting with the\ntrim_zscore combination, since we achieved the highest precision, recall, and F1-score metrics for that condi-\ntion. We used both the SVM and RF models trained on the lab dataset for classification in the field. Needless\nto say, the field data went through the same preprocessing methods, in this case, trimming outliers followed by\nz-score normalization. The results are shown in Table 14. We observe that removing windows with high physical\nactivity greatly improved the prediction results, leading to a maximum F1-score of 0.62 when we used SVM for", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.161087"}}
{"text": "prediction. Further, by using our two-layer modeling approach, we observed that the F1-score improved to 0.66.\nAlthough the field F1-score reported by our model might seem low, it needs to be considered that we are using\njust a commodity device; unlike previous works that have used high-quality sensors, and fused it with other data\nsources like respiration (in cStress [32]) or GSR, and skin temperature (by Gjoreski et al. [25]), and attain field\nF1-scores of 0.71 and 0.63, respectively, comparable to our results.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.161314"}}
{"text": "We show that using just a commodity heart rate sensor with a rigorous data processing and feature selection\npipeline, we can accurately infer stress as well as (if not better) than using an ECG device. In previous work,\nsuch as cStress, the authors [32] showed that using just the ECG data, they could infer stress in the lab with\nan F1-score of 0.78, compared to an F1-score of 0.87 in our case, as shown in Figure 7. They do not report field\nresults using just the ECG sensor, so we compare our field results to a biased random classifier as the baseline.\nThe baseline classifier randomly classifies each instance between 0 or 1, based on the probability distribution of\nthe training set, and yields an F1-score of 0.44 in the field. Our approach achieves 52% better results than the\nbaseline.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.161538"}}
{"text": "Although we show initial evidence that commodity heart rate sensors (at least the Polar H7) can be used for\nstress detection, there is still room for improvement. We anticipate that an increase in the sensor quality and\ntraining for a wider (and a more varied) range of stress-inducing tasks could see an increase in the inference\nresults. In the meantime, we believe that researchers might supplement the heart rate data with other physiolog-\nical data (1) to improve the accuracy or (2) to capture and compare the effect of different physiological signals in\nstress monitoring. Hence, it is important that our data processing pipeline works for other sensor data streams.\nTo this end, we evaluate how well our model performs when we combine data from a GSR device to the data\ncollected from the Polar H7.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.161764"}}
{"text": "In our study, we also asked the participants to wear a custom-made GSR sensor. We were able to record lab\nand field GSR data from 15 of the 27 participants in the study. We use the heart rate and GSR data from these\n15 participants to build a new combined model and report the change in classification results. The GSR sensor\nused in this work had similar technical specifications as the one developed and evaluated by Pope et al. [44] and\ncould measure electrodermal activity at the ventral wrist for a range of skin conductance values between 0.24\nand 6.0μS .", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.162009"}}
{"text": "The GSR data we collected also undergoes the same rigorous data cleaning and preprocessing steps; as before,\nwe had six different combinations of outlier handling and normalization, as shown in Table 4. Note, however,\nthat these combinations now contain both heart rate and GSR data for 15 participants.9 As with the heart rate\ndata (which we now refer to as HR data), we follow a similar approach for the merged heart rate and GSR data", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.162242"}}
{"text": "9Since the goal of this work is to evaluate how a commodity heart rate monitor works for stress measurement, we look at the combination\nof heart rate and GSR data. We do not report results or draw comparisons about the performance of just the GSR sensor, as it is beyond the\nscope of this work.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.162472"}}
{"text": "(which we now refer to as HR-GSR data), starting with feature computation, followed by observing significant\ndifferences, lab data results, and finally the field data results.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.162713"}}
{"text": "5.4.1 Features for HR-GSR Data. There are two main components to the overall GSR signal. The tonic com-\nponent relates to the slower-acting components and background characteristics of the signal—that is, the overall\nlevel, slow rise, or declines over time. The common measure for the tonic component is the skin conductance\nlevel (SCL), and changes in SCL are known to reflect changes in arousal in the autonomic nervous system. For\neach window, we used the mean, max, min, and standard deviation of the SCL as features. The second component\nof the GSR signal is called the phasic component, which represents the faster-changing elements of the signal\nand is measured by skin reductance response (SCR) [11]. For each window, we compute the total number of SCRs\nin the window (total_SCR), sum of amplitude of the SCRs (sum_amp), sum of SCR durations (sum_dur), and the\ntotal SCR area (auc_SCR). For these latter computations, we use the EDA Explorer tool (with threshold = 0.05μS)\nmade available by Taylor et al. [52]. Table 15 lists the complete set of GSR features that, in addition to the heart\nrate features computed earlier, were used for the HR-GSR data.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.162966"}}
{"text": "5.4.2 Capturing Significant Difference Using GSR Data. In Section 5.1, we showed that the features computed\nby the heart rate sensor exhibit statistically significant differences between the baseline rest period and the stress\ninduction periods. We also hypothesized that there might be some residual stress that is being exhibited in the\ninitial minutes of the rest period, and by considering the last 4 minutes of the initial rest period as the baseline,\nwe observed more features that exhibited significant difference. A similar comparison using features computed\nwith the GSR data would help us validate whether our hypothesis was in fact true.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.163205"}}
{"text": "As in Section 5.1, we report the Welch’s t-test result for the trim_zscore dataset. The results of the t-test using\nthe GSR features, where we consider the entire 10 minutes of the initial rest period as the baseline, are shown in\nTable 16. We observe that only two and one features show significant difference for the book test and the cold\ntest, respectively, suggesting that the GSR features are not able to capture differences between the baseline (of\n10 minutes) and stress induction periods. Next we look at the t-test results by considering only the last 4 minutes\nof the initial rest period as the baseline (shown in Table 17). It is evident that more features exhibit significant", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.163437"}}
{"text": "5.4.3 Evaluation in the Lab. To evaluate the HR-GSR datasets in the lab, we follow an approach similar to the\nlab evaluation in Section 5.2. In this section, we report the results for a LOSO cross validation from the different\ndatasets, using SVM and RF, while considering only the last 4 minutes of the initial rest period as not stressed.\nThe results obtained are shown in Table 18. We observe an increase in the F1-score, once we include the GSR\ndata. Although the best results obtained were from trim_zscore (as for HR-only data), it is interesting to see that\nan RF model performed better than SVM for HR-GSR data (whereas SVM was better for HR data). Next, we used\nthis RF model with our proposed two-layer model and observed that the F1-score improved to 0.94.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.163670"}}
{"text": "As for HR-only data, we report the feature importance results for the HR-GSR data in Figure 8. From the\nfeature importance plots, we observe that the heart rate features (obtained from a commodity sensor) did play\nan important role in the overall classification model, even when combined with a custom sensor, which suggests\nthat using a custom sensor does not obviate the need for the heart rate sensor.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.163911"}}
{"text": "5.4.4 Evaluation in the Field. As in the field evaluation of the HR-only data, we use the model built during the\nlab evaluation of the HR-GSR data to predict the stress labels in the field study. We report the results in Table 19.\nWe observe that combination of GSR data and HR data results in an F1-score of 0.70, which improves to 0.73 when\nconsidered in conjunction with the Bayesian network model in our proposed two-layer approach. Based on this\nresult, it seems that trim_zscore can be used as a standard data processing step for accurately detecting stress\nfrom a commodity heart rate monitor, and it can be extended to other sensor streams being used in combination\n(GSR in our case), with similar levels of classification performance as the clinical-grade or custom sensor–based\nsystems used by other researchers in the past.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.164148"}}
{"text": "Fig. 8. Feature importance representation using RF and linear SVM, using the trim_zscore combination of HR-GSR data,", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.164382"}}
{"text": "Capturing context. Currently, we infer stress with data from a commercial heart rate sensor. However, re-\nsearchers have shown that contextual information can also be useful for inferring stress [37]. With the popu-\nlarity of smartphones and smartwatches, and with the availability of multiple sensors on these devices, context\nmonitoring has become relatively straightforward. In the future, we intend to augment the data obtained from\nthe heart ate sensor with contextual information to identify stress and nonstress periods. If we find that con-\ntextual markers help in identifying stress, context may also be used to predict a stressful situation before it\noccurs. This, in turn, will open new research directions. In our work, we use one aspect about the context of\nthe user—activity, which helps us remove instances in the physiological signals that could be caused by physical\nactivity. Although we use an Amulet to measure activity and administer EMAs, we anticipate that other mobile\nand wearable devices may also be suitable for detecting motion or physical activity context.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.164672"}}
{"text": "Standardizing the processing pipeline. Prior research using clinical-grade sensors to monitor stress indicates that\nwinsorization is an effective preprocessing approach to handling outliers [26, 32]. For commodity, off-the-shelf\nsensors (Polar H7), we found that the results obtained after performing simple trimming outperformed the results\nobtained after performing winsorization. This observation indicates that data preprocessing and data cleaning\ntechniques that perform well for clinical-grade devices might not perform equally well for commodity devices.\nOne possible explanation for the difference might be the noise present in the data. Since clinical-grade sensors\nare more robust than their consumer-grade counterparts, we expect that they are less prone to environmental\nnoise. It may be that the outliers present in the data obtained from such clinical devices might have some useful\ninformation, making winsorization an effective method. This example is one that could be directly compared to\nprevious works; there might be more such instances where choices for data processing and data modeling might\nlead to differing performance in commodity and clinical devices. Based on our analyses, however, it seems that\ntrimming along with z-score normalization might be a better way to process data for the task of stress detection.\nThe proposed steps led to the best classification results for both HR-only data and HR-GSR data, coming from\ncommodity and custom-made devices. This suggests that our processing pipeline is robust for two device types\nand may be appropriate for other sensor streams as well.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.164908"}}
{"text": "Usability of chest-based sensors. Some believe that usually chest-band-based sensors might be bulky and un-\ncomfortable, making them a poor choice for continuous usage. We chose to use the Polar H7, a very popular\nproduct, with more than 2,300 five-star reviews on Amazon.10 Further, in our study, after the field session was\nconcluded, we conducted a quantitative and qualitative survey about how the participants felt about our data\ncollection system. When asked if they “found the system to cause physical discomfort,” participants mostly dis-\nagreed (mean = 1.22 on a scale from 0 to 4, with 0 being strongly disagreed and 4 being strongly agreed); only\nthree participants agreed that the device caused discomfort. On being asked if they “could have worn the system\nfor a longer period of time,” participants mostly agreed (mean = 2.81). In fact, participants were more concerned\nabout the comfort factor of the wrist-based GSR device. Almost 50% of the participants noted they found the\nGSR device to cause physical discomfort—we believe that this was because of the protruding electrodes.", "meta": {"corpora_id": "acm1.2020-05-22 15:28:34.165144"}}