{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition for dataset names\n",
    "\n",
    "## Date: March 9, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;1m✘ 'dataset_name' already exists in database SQLite\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!prodigy dataset dataset_name 'Seed terms for DATASET label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prodigy terms.teach dataset_name en_core_web_lg --seeds ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import json\n",
    "import pandas as pd\n",
    "from os.path import isfile, join\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the list of file names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'labeled_sample'\n",
    "nme = [f for f in listdir(path) if isfile(join(path, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data from the json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = []\n",
    "lbl = []\n",
    "for nm in nme:\n",
    "    try:\n",
    "        with open('labeled_sample/{}'.format(nm), encoding='windows-1252') as f:\n",
    "            dct = json.load(f)\n",
    "            txt.append(list(dct.keys())[0])\n",
    "            lbl.append(list(dct.values())[0])\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'text': txt,\n",
    "    'label': lbl\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf = pd.DataFrame(d, columns=['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>International Journal of Commun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>International Journal of Computer Applications...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>816. Conclusions  This thesis described the t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                 International Journal of Commun...      1\n",
       "1  International Journal of Computer Applications...      0\n",
       "2   816. Conclusions  This thesis described the t...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting only the positive labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_true = dtf[dtf.label==1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PAGE 38MULTIFAMILY GREEN BUILDING GUIDELINESMEASURE AA7PASSIVE SOLAR DESIGN, DAYLIGHTING AND NATURAL VENTILATIONConsiderations for ResidentsResidents may beneﬁt from reduced heating and cooling bills and better indoor environmental quality. Teachresidents about the strategies used so that they don’t unintentionally circumvent the design. A throw rug, for example, will reduce a mass ﬂoor’s ability to store heat. Cost and Cost EffectivenessBENEFITCOSTThe strategies in this measure may increase design time. Passive solar design may increase material costs for items such as shading devices or extra concrete or drywall for thermal mass. However, passive design strategies are probably the best way to reduce ﬁrst costs associated with system sizing (Systems: H0–Heating Equipment) and provide ongoing savings throughout the building’s life. A basic level of daylighting and natural ventilation can usually be provided for no increase over standard construction costs. Skylights, clerestories, tall windows, cupolas, deep overhangs, awnings and other nonstandard design features may increase costs. Strategies more common in ofﬁce buildings, such as light shelves, automatic lighting controls and specialized glazing, can signiﬁcantly increase costs.Resources»California’s utility companies provide resources for passive systems design, including modeling tools, solar calculators and climate data. Check with your utility.  »GreenAffordable Housing Coalition has factsheets on passive solar design and daylighting for affordable housing: www.greenaffordablehousing.org » National Oceanographic and AtmosphericAdministration (NOAA) has climate data including design temperatures, degree-day averages, and more: www.noaa.gov» NationalRenewableEnergyLaboratory provides solar insolation values:www.nrel.gov/rredc» U.S.Department of Energy offers passive solar and daylighting fundamentals: www.eere.energy.gov/buildings/info Related Case Studies»Carmen Avenue, p. 230»Crossroads, p. 234»Sara Conner Court Apartments, p. 221PLANNING & DESIGN'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf_true.loc[6, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a set a subsets where we we have the sentence with the mention of \"%data%\" plus the sentence before (if it exists), plus the sentence after (if it exists).\n",
    "\n",
    "- Extracting one sentence before and one sentence after the mention of data (3 sentences subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = dtf_true.loc[6, 'text']\n",
    "a_list = nltk.tokenize.sent_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = []\n",
    "def get_three_sentences_subsets(txt):\n",
    "    stc = nltk.tokenize.sent_tokenize(txt)\n",
    "    for i, st in enumerate(stc):\n",
    "        if 'data' in st and i==0:\n",
    "            sub.append(' '.join([st, stc[1]]))\n",
    "        elif 'data' in st and i!=len(stc)-1:\n",
    "            sub.append(' '.join([stc[i-1], st, stc[i+1]]))\n",
    "        elif 'data' in st and i==len(stc)-1:\n",
    "            sub.append(' '.join([stc[i-1], st]))\n",
    "        else: pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(get_three_sentences_subsets, list(dtf_true.text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with 148 subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a subset that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Then it uses the pre-processed signal to extract features from the ECG signal automatically. The ECG signals for the present study are collected from MIT/BIH database via Physionet database and stored in a text format. The MIT-BIH database contains different types of Electrocardiogram signals including both abnormal Electrocardiograms and normal Electrocardiograms, which are sampled at different rates.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to identify for each of the subsets (if it exists):\n",
    "- the name of the mentioned dataset\n",
    "- the source of the mentioned dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save our subset into a json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = [{'text': txt} for txt in sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('prodigy'):\n",
    "    os.mkdir('prodigy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prodigy/ner_text_to_label.json', 'w') as f:\n",
    "    json.dump(json_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('prodigy/ner_text_to_label.json', 'w') as f:\n",
    "#    dat_to_label = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 label(s): dataset_name\n",
      "Added dataset json_list to database SQLite.\n",
      "\n",
      "✨  Starting the web server at http://localhost:8080 ...\n",
      "Open the app in your browser and start annotating!\n",
      "\n",
      "\n",
      "\u001b[38;5;1m✘ Can't find file /tmp/ner_datat.dump\u001b[0m\n",
      "/private/tmp/ner_datat.dump\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!prodigy ner.manual json_list en_core_web_sm /tmp/ner_datat.dump --loader txt --label dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
